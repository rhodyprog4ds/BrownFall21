{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d106952f",
   "metadata": {},
   "source": [
    "# More NLP & Solving problems with ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c20b66",
   "metadata": {},
   "source": [
    "Imagine you work for a news agency, current articles are stored in a database in\n",
    "a table where they are indexed by a unique identifier. A separate table indicates\n",
    "the primary category for most articles, but some are missing. Your assignment is to create an automatic category generator to save editors time. Your manager suggests that\n",
    "an SVM or decision tree is probably best, but is unsure what text representations\n",
    "will perform best. You are expected to produce an accessible report with tables and plots that visualize the performance and impact of various modeling choices. \n",
    "\n",
    "\n",
    "\n",
    "Write the import statements you would need to solve this problem. Include\n",
    "whole libraries or modules as is most appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851b44b",
   "metadata": {},
   "source": [
    "```python \n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fceab",
   "metadata": {},
   "source": [
    "Given the following vocabulary,\n",
    "\n",
    "['and', 'are', 'cat', 'cats', 'dogs', 'pets', 'popular', 'videos']\n",
    "\n",
    "\n",
    "represent \"Cats and dogs are pets\" in vector format.\n",
    "\n",
    "[1,1,0,1,1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cefaf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import a python utility for examingin ojbects for the notes\n",
    "from sys import getsizeof\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "ng_X,ng_y = datasets.fetch_20newsgroups(categories =['comp.graphics','sci.crypt'],\n",
    "                    return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27b4614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ng_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca56697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64be576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: robert@cpuserver.acsc.com (Robert Grant)\\nSubject: Virtual Reality for X on the CHEAP!\\nOrganization: USCACSC, Los Angeles\\nLines: 187\\nDistribution: world\\nReply-To: robert@cpuserver.acsc.com (Robert Grant)\\nNNTP-Posting-Host: cpuserver.acsc.com\\n\\nHi everyone,\\n\\nI thought that some people may be interested in my VR\\nsoftware on these groups:\\n\\n*******Announcing the release of Multiverse-1.0.2*******\\n\\nMultiverse is a multi-user, non-immersive, X-Windows based Virtual Reality\\nsystem, primarily focused on entertainment/research.\\n\\nFeatures:\\n\\n   Client-Server based model, using Berkeley Sockets.\\n   No limit to the number of users (apart from performance).\\n   Generic clients.\\n   Customizable servers.\\n   Hierachical Objects (allowing attachment of cameras and light sources).\\n   Multiple light sources (ambient, point and spot).\\n   Objects can have extension code, to handle unique functionality, easily\\n        attached.\\n\\nFunctionality:\\n\\n  Client:\\n   The client is built around a 'fast' render loop. Basically it changes things\\n   when told to by the server and then renders an image from the user's\\n   viewpoint. It also provides the server with information about the user's\\n   actions - which can then be communicated to other clients and therefore to\\n   other users.\\n\\n   The client is designed to be generic - in other words you don't need to\\n   develop a new client when you want to enter a new world. This means that\\n   resources can be spent on enhancing the client software rather than adapting\\n   it. The adaptations, as will be explained in a moment, occur in the servers.\\n\\n   This release of the client software supports the following functionality:\\n\\n    o Hierarchical Objects (with associated addressing)\\n\\n    o Multiple Light Sources and Types (Ambient, Point and Spot)\\n\\n    o User Interface Panels\\n\\n    o Colour Polygonal Rendering with Phong Shading (optional wireframe for\\n\\tfaster frame rates)\\n\\n    o Mouse and Keyboard Input\\n\\n   (Some people may be disappointed that this software doesn't support the\\n   PowerGlove as an input device - this is not because it can't, but because\\n   I don't have one! This will, however, be one of the first enhancements!)\\n\\n  Server(s):\\n   This is where customization can take place. The following basic support is\\n   provided in this release for potential world server developers:\\n\\n    o Transparent Client Management\\n\\n    o Client Message Handling\\n\\n   This may not sound like much, but it takes away the headache of\\naccepting and\\n   terminating clients and receiving messages from them - the\\napplication writer\\n   can work with the assumption that things are happening locally.\\n\\n   Things get more interesting in the object extension functionality. This is\\n   what is provided to allow you to animate your objects:\\n\\n    o Server Selectable Extension Installation:\\n        What this means is that you can decide which objects have extended\\n        functionality in your world. Basically you call the extension\\n        initialisers you want.\\n\\n    o Event Handler Registration:\\n        When you develop extensions for an object you basically write callback\\n        functions for the events that you want the object to respond to.\\n        (Current events supported: INIT, MOVE, CHANGE, COLLIDE & TERMINATE)\\n\\n    o Collision Detection Registration:\\n        If you want your object to respond to collision events just provide\\n        some basic information to the collision detection management software.\\n        Your callback will be activated when a collision occurs.\\n\\n    This software is kept separate from the worldServer applications because\\n    the application developer wants to build a library of extended objects\\n    from which to choose.\\n\\n    The following is all you need to make a World Server application:\\n\\n    o Provide an initWorld function:\\n        This is where you choose what object extensions will be supported, plus\\n        any initialization you want to do.\\n\\n    o Provide a positionObject function:\\n        This is where you determine where to place a new client.\\n\\n    o Provide an installWorldObjects function:\\n        This is where you load the world (.wld) file for a new client.\\n\\n    o Provide a getWorldType function:\\n        This is where you tell a new client what persona they should have.\\n\\n    o Provide an animateWorld function:\\n        This is where you can go wild! At a minimum you should let the objects\\n        move (by calling a move function) and let the server sleep for a bit\\n        (to avoid outrunning the clients).\\n\\n    That's all there is to it! And to prove it here are the line counts for the\\n    three world servers I've provided:\\n\\n        generic - 81 lines\\n        dactyl - 270 lines (more complicated collision detection due to the\\n                           stairs! Will probably be improved with future\\n                           versions)\\n        dogfight - 72 lines\\n\\nLocation:\\n\\n   This software is located at the following site:\\n   ftp.u.washington.edu\\n\\n   Directory:\\n   pub/virtual-worlds\\n\\n   File:\\n   multiverse-1.0.2.tar.Z\\n\\nFutures:\\n\\n   Client:\\n\\n    o Texture mapping.\\n\\n    o More realistic rendering: i.e. Z-Buffering (or similar), Gouraud shading\\n\\n    o HMD support.\\n\\n    o Etc, etc....\\n\\n   Server:\\n\\n    o Physical Modelling (gravity, friction etc).\\n\\n    o Enhanced Object Management/Interaction\\n\\n    o Etc, etc....\\n\\n   Both:\\n\\n    o Improved Comms!!!\\n\\nI hope this provides people with a good understanding of the Multiverse\\nsoftware,\\nunfortunately it comes with practically zero documentation, and I'm not sure\\nwhether that will ever be able to be rectified! :-(\\n\\nI hope people enjoy this software and that it is useful in our explorations of\\nthe Virtual Universe - I've certainly found fascinating developing it, and I\\nwould *LOVE* to add support for the PowerGlove...and an HMD :-)!!\\n\\nFinally one major disclaimer:\\n\\nThis is totally amateur code. By that I mean there is no support for this code\\nother than what I, out the kindness of my heart, or you, out of pure\\ndesperation, provide. I cannot be held responsible for anything good or bad\\nthat may happen through the use of this code - USE IT AT YOUR OWN RISK!\\n\\nDisclaimer over!\\n\\nOf course if you love it, I would like to here from you. And anyone with\\nPOSITIVE contributions/criticisms is also encouraged to contact me. Anyone who\\nhates it: > /dev/null!\\n\\n************************************************************************\\n*********\\nAnd if anyone wants to let me do this for a living: you know where to\\nwrite :-)!\\n************************************************************************\\n*********\\n\\nThanks,\\n\\nRobert.\\n\\nrobert@acsc.com\\n^^^^^^^^^^^^^^^\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7de8fd",
   "metadata": {},
   "source": [
    "We're going to instantiate the object and fit it two the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fae570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = text.CountVectorizer()\n",
    "\n",
    "count_vec.fit(ng_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f77b2",
   "metadata": {},
   "source": [
    "```{important}\n",
    "I changed the following a little bit from class so that we can use the same test/train split for the two different types of transformation so that we can compare them more easily. \n",
    "\n",
    "This also helps illustrate when using the fit and transform separately is helpful. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef58e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_X_train, ng_X_test, ng_y_train, ng_y_test = train_test_split(\n",
    "                                        ng_X, ng_y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9da2b1",
   "metadata": {},
   "source": [
    "Now, we can use the transformation that we fit to the whole dataset to transform the train and test portions of the data separately. \n",
    "\n",
    "The transform method also returns the sparse matrix directly so we no longer need the `toarray` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f50f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_vec_train = count_vec.transform(ng_X_train)\n",
    "ng_vec_test = count_vec.transform(ng_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f681772e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24257 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 84 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_vec_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d77d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af07b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9830508474576272"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(ng_vec_train,ng_y_train).score(ng_vec_test,ng_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d388817b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[\"From: robert@cpuserver.acsc.com (Robert Grant)\\nSubject: Virtual Reality for X on the CHEAP!\\nOrganization: USCACSC, Los Angeles\\nLines: 187\\nDistribution: world\\nReply-To: robert@cpuserver.acsc.com (Robert Grant)\\nNNTP-Posting-Host: cpuserver.acsc.com\\n\\nHi everyone,\\n\\nI thought that some people may be interested in my VR\\nsoftware on these groups:\\n\\n*******Announcing the release of Multiverse-1.0.2*******\\n\\nMultiverse is a multi-user, non-immersive, X-Windows based Virtual Reality\\nsystem, primarily focused on entertainment/research.\\n\\nFeatures:\\n\\n   Client-Server based model, using Berkeley Sockets.\\n   No limit to the number of users (apart from performance).\\n   Generic clients.\\n   Customizable servers.\\n   Hierachical Objects (allowing attachment of cameras and light sources).\\n   Multiple light sources (ambient, point and spot).\\n   Objects can have extension code, to handle unique functionality, easily\\n        attached.\\n\\nFunctionality:\\n\\n  Client:\\n   The client is built around a 'fast' render loop. Basically it changes things\\n   when told to by the server and then renders an image from the user's\\n   viewpoint. It also provides the server with information about the user's\\n   actions - which can then be communicated to other clients and therefore to\\n   other users.\\n\\n   The client is designed to be generic - in other words you don't need to\\n   develop a new client when you want to enter a new world. This means that\\n   resources can be spent on enhancing the client software rather than adapting\\n   it. The adaptations, as will be explained in a moment, occur in the servers.\\n\\n   This release of the client software supports the following functionality:\\n\\n    o Hierarchical Objects (with associated addressing)\\n\\n    o Multiple Light Sources and Types (Ambient, Point and Spot)\\n\\n    o User Interface Panels\\n\\n    o Colour Polygonal Rendering with Phong Shading (optional wireframe for\\n\\tfaster frame rates)\\n\\n    o Mouse and Keyboard Input\\n\\n   (Some people may be disappointed that this software doesn't support the\\n   PowerGlove as an input device - this is not because it can't, but because\\n   I don't have one! This will, however, be one of the first enhancements!)\\n\\n  Server(s):\\n   This is where customization can take place. The following basic support is\\n   provided in this release for potential world server developers:\\n\\n    o Transparent Client Management\\n\\n    o Client Message Handling\\n\\n   This may not sound like much, but it takes away the headache of\\naccepting and\\n   terminating clients and receiving messages from them - the\\napplication writer\\n   can work with the assumption that things are happening locally.\\n\\n   Things get more interesting in the object extension functionality. This is\\n   what is provided to allow you to animate your objects:\\n\\n    o Server Selectable Extension Installation:\\n        What this means is that you can decide which objects have extended\\n        functionality in your world. Basically you call the extension\\n        initialisers you want.\\n\\n    o Event Handler Registration:\\n        When you develop extensions for an object you basically write callback\\n        functions for the events that you want the object to respond to.\\n        (Current events supported: INIT, MOVE, CHANGE, COLLIDE & TERMINATE)\\n\\n    o Collision Detection Registration:\\n        If you want your object to respond to collision events just provide\\n        some basic information to the collision detection management software.\\n        Your callback will be activated when a collision occurs.\\n\\n    This software is kept separate from the worldServer applications because\\n    the application developer wants to build a library of extended objects\\n    from which to choose.\\n\\n    The following is all you need to make a World Server application:\\n\\n    o Provide an initWorld function:\\n        This is where you choose what object extensions will be supported, plus\\n        any initialization you want to do.\\n\\n    o Provide a positionObject function:\\n        This is where you determine where to place a new client.\\n\\n    o Provide an installWorldObjects function:\\n        This is where you load the world (.wld) file for a new client.\\n\\n    o Provide a getWorldType function:\\n        This is where you tell a new client what persona they should have.\\n\\n    o Provide an animateWorld function:\\n        This is where you can go wild! At a minimum you should let the objects\\n        move (by calling a move function) and let the server sleep for a bit\\n        (to avoid outrunning the clients).\\n\\n    That's all there is to it! And to prove it here are the line counts for the\\n    three world servers I've provided:\\n\\n        generic - 81 lines\\n        dactyl - 270 lines (more complicated collision detection due to the\\n                           stairs! Will probably be improved with future\\n                           versions)\\n        dogfight - 72 lines\\n\\nLocation:\\n\\n   This software is located at the following site:\\n   ftp.u.washington.edu\\n\\n   Directory:\\n   pub/virtual-worlds\\n\\n   File:\\n   multiverse-1.0.2.tar.Z\\n\\nFutures:\\n\\n   Client:\\n\\n    o Texture mapping.\\n\\n    o More realistic rendering: i.e. Z-Buffering (or similar), Gouraud shading\\n\\n    o HMD support.\\n\\n    o Etc, etc....\\n\\n   Server:\\n\\n    o Physical Modelling (gravity, friction etc).\\n\\n    o Enhanced Object Management/Interaction\\n\\n    o Etc, etc....\\n\\n   Both:\\n\\n    o Improved Comms!!!\\n\\nI hope this provides people with a good understanding of the Multiverse\\nsoftware,\\nunfortunately it comes with practically zero documentation, and I'm not sure\\nwhether that will ever be able to be rectified! :-(\\n\\nI hope people enjoy this software and that it is useful in our explorations of\\nthe Virtual Universe - I've certainly found fascinating developing it, and I\\nwould *LOVE* to add support for the PowerGlove...and an HMD :-)!!\\n\\nFinally one major disclaimer:\\n\\nThis is totally amateur code. By that I mean there is no support for this code\\nother than what I, out the kindness of my heart, or you, out of pure\\ndesperation, provide. I cannot be held responsible for anything good or bad\\nthat may happen through the use of this code - USE IT AT YOUR OWN RISK!\\n\\nDisclaimer over!\\n\\nOf course if you love it, I would like to here from you. And anyone with\\nPOSITIVE contributions/criticisms is also encouraged to contact me. Anyone who\\nhates it: > /dev/null!\\n\\n************************************************************************\\n*********\\nAnd if anyone wants to let me do this for a living: you know where to\\nwrite :-)!\\n************************************************************************\\n*********\\n\\nThanks,\\n\\nRobert.\\n\\nrobert@acsc.com\\n^^^^^^^^^^^^^^^\\n\"\n \"From: lulagos@cipres.cec.uchile.cl (admirador)\\nSubject: OAK VGA 1Mb. Please, I needd VESA TSR!!! 8^)\\nOriginator: lulagos@cipres\\nNntp-Posting-Host: cipres.cec.uchile.cl\\nOrganization: Centro de Computacion (CEC), Universidad de Chile\\nLines: 15\\n\\n\\n\\tHi there!...\\n\\t\\tWell, i have a 386/40 with SVGA 1Mb. (OAK chip 077) and i don't\\n\\t\\thave VESA TSR program for this card. I need it . \\n\\t\\t\\tPlease... if anybody can help me, mail me at:\\n\\t\\t\\tlulagos@araucaria.cec.uchile.cl\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThanks.\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMackk. \\n   _   /| \\n   \\\\'o.O' \\n   =(___)=\\n      U   \\n     Ack!\\n\"\n 'From: wcs@anchor.ho.att.com (Bill Stewart +1-908-949-0705)\\nSubject: Re: Clipper considered harmful\\nOrganization: Sorcerer\\'s Apprentice Cleaning Services\\nDistribution: inet\\nIn-Reply-To: shirriff@sprite.berkeley.edu\\'s message of 21 Apr 1993 00:36:44 GMT\\nNntp-Posting-Host: rainier.ho.att.com\\nLines: 24\\n\\nIn article <1r24us$oeh@agate.berkeley.edu> shirriff@sprite.berkeley.edu (Ken Shirriff) writes:\\n   In article <15469@optilink.COM> brad@optilink.COM (Brad Yearwood) writes:\\n   >Finally, because there is essentially no possibility of intercepting in\\n   >realtime the scrutable content of communications between stolen instruments,\\n   >there will exist strong motivation to record and archive _all_ communications\\n   >in the network for ex-post-facto scrutiny (once some criminal act is\\n   >discovered, and the instruments involved have been identified).\\n\\n\"All\" is a *very* big number.  The AT&T Long Distance network has\\naround 20,000 T3 trunks (45 Mbit/sec), which is on the order of 10**12 bits/sec.\\nThat doesn\\'t even count the amount of traffic in the LOCAL phone companies,\\nor our long-distance competitors.  It\\'s about 200 Exabytes tapes / second,\\nwhich is pretty large even for the NSA :-)\\n\\nOn the other hand, I can easily see them recording the traffic for\\n\"interesting\" people, such as dissidents, suspected criminals,\\nforeign telephone calls, and anybody noticed using encryption.\\nAs Ken Shiriff speculates, recording encrypted traffic will probably\\nbe judged not to be an invasion of privacy pretty soon ....\\n--\\n#\\t\\t\\t\\tPray for peace;      Bill\\n# Bill Stewart 1-908-949-0705 wcs@anchor.att.com AT&T Bell Labs 4M312 Holmdel NJ\\n#\\t              No, I\\'m *from* New Jersey, I only *work* in cyberspace....\\n# White House Commect Line 1-202-456-1111  fax 1-202-456-2461\\n'\n ...\n \"From: gtoal@gtoal.com (Graham Toal)\\nSubject: Re: PGP ideas for IBM systems\\nLines: 7\\n\\n:    I've been thinking about how difficult it would be to make PGP available\\n: in some form on EBCDIC machines.\\n\\nDon't encourage them.  Let EBCDIC machines die an honorable death :)\\n\\nG\\n\\n\"\n \"From: doug@hparc0.aus.hp.com (Doug Parsons)\\nSubject: Re: 3d-Studio V2.01 : Any differences with previous version\\nOrganization: HP Australasian Response Centre (Melbourne)\\nX-Newsreader: TIN [version 1.1 PL8.5]\\nLines: 10\\n\\nFOMBARON marc (fombaron@ufrima.imag.fr) wrote:\\n: Are there significant differences between V2.01 and V2.00 ?\\n: Thank you for helping\\n\\n\\nNo.  As I recall, the only differences are in the 3ds.set parameters - some\\nof the defaults have changed slightly.  I'll look when I get home and let\\nyou know, but there isn't enough to actually warrant upgrading.\\n\\ndouginoz\\n\"\n 'From: rboudrie@chpc.org (Rob Boudrie)\\nSubject: Re: Once tapped, your code is no good any more.\\nOrganization: Center For High Perf. Computing of WPI; Marlboro Ma\\nDistribution: na\\nLines: 26\\n\\nIn article <C5so84.Hxv@demon.co.uk> Graham Toal <gtoal@gtoal.com> writes:\\n>In article <2073@rwing.UUCP> pat@rwing.UUCP (Pat Myrto) writes:\\n>:If the Clinton Clipper is so very good, why not make its algrithm public\\n>:so many people can exchange ideas and examine it, rather than a few\\n>:isolated \\'respected experts\\' (respected by whom?  for what?  Perhaps a\\n\\nOne more time...\\n\\n    If they released the algorithm, it would be possible for someone\\n    to come up with an implementation which was identical, but\\n    lacking an escrowed key.\\n\\n    Note that the press announcement mentioned that the algorithm was\\n    being kept secret for security of the key escrow system.  In this\\n    case security means \"an escrowed key for EVERY clipper chip\".\\n\\n\\n    Assuming you believed all that is said about the effective of\\n    the algorithm, and the escrow system, which would you buy :\\n\\n     (a)  Chip from firm A with the escrowed key\\n     (b)  Second source chip from reputable firm B with no key\\n          in government escrow.\\n\\n    There would obviously be powerful economic incentives for a second\\n    source, non escrowed, vendor.\\n'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2456/2043128607.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[0;31m# dtype NPY_INTP which is int32 for 32bit platforms. See #20923\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m         X = self._validate_data(\n\u001b[0;32m-> 1615\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1616\u001b[0m         )\n\u001b[1;32m   1617\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    763\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                     \u001b[0;34m\"if it contains a single sample.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m                 )\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[\"From: robert@cpuserver.acsc.com (Robert Grant)\\nSubject: Virtual Reality for X on the CHEAP!\\nOrganization: USCACSC, Los Angeles\\nLines: 187\\nDistribution: world\\nReply-To: robert@cpuserver.acsc.com (Robert Grant)\\nNNTP-Posting-Host: cpuserver.acsc.com\\n\\nHi everyone,\\n\\nI thought that some people may be interested in my VR\\nsoftware on these groups:\\n\\n*******Announcing the release of Multiverse-1.0.2*******\\n\\nMultiverse is a multi-user, non-immersive, X-Windows based Virtual Reality\\nsystem, primarily focused on entertainment/research.\\n\\nFeatures:\\n\\n   Client-Server based model, using Berkeley Sockets.\\n   No limit to the number of users (apart from performance).\\n   Generic clients.\\n   Customizable servers.\\n   Hierachical Objects (allowing attachment of cameras and light sources).\\n   Multiple light sources (ambient, point and spot).\\n   Objects can have extension code, to handle unique functionality, easily\\n        attached.\\n\\nFunctionality:\\n\\n  Client:\\n   The client is built around a 'fast' render loop. Basically it changes things\\n   when told to by the server and then renders an image from the user's\\n   viewpoint. It also provides the server with information about the user's\\n   actions - which can then be communicated to other clients and therefore to\\n   other users.\\n\\n   The client is designed to be generic - in other words you don't need to\\n   develop a new client when you want to enter a new world. This means that\\n   resources can be spent on enhancing the client software rather than adapting\\n   it. The adaptations, as will be explained in a moment, occur in the servers.\\n\\n   This release of the client software supports the following functionality:\\n\\n    o Hierarchical Objects (with associated addressing)\\n\\n    o Multiple Light Sources and Types (Ambient, Point and Spot)\\n\\n    o User Interface Panels\\n\\n    o Colour Polygonal Rendering with Phong Shading (optional wireframe for\\n\\tfaster frame rates)\\n\\n    o Mouse and Keyboard Input\\n\\n   (Some people may be disappointed that this software doesn't support the\\n   PowerGlove as an input device - this is not because it can't, but because\\n   I don't have one! This will, however, be one of the first enhancements!)\\n\\n  Server(s):\\n   This is where customization can take place. The following basic support is\\n   provided in this release for potential world server developers:\\n\\n    o Transparent Client Management\\n\\n    o Client Message Handling\\n\\n   This may not sound like much, but it takes away the headache of\\naccepting and\\n   terminating clients and receiving messages from them - the\\napplication writer\\n   can work with the assumption that things are happening locally.\\n\\n   Things get more interesting in the object extension functionality. This is\\n   what is provided to allow you to animate your objects:\\n\\n    o Server Selectable Extension Installation:\\n        What this means is that you can decide which objects have extended\\n        functionality in your world. Basically you call the extension\\n        initialisers you want.\\n\\n    o Event Handler Registration:\\n        When you develop extensions for an object you basically write callback\\n        functions for the events that you want the object to respond to.\\n        (Current events supported: INIT, MOVE, CHANGE, COLLIDE & TERMINATE)\\n\\n    o Collision Detection Registration:\\n        If you want your object to respond to collision events just provide\\n        some basic information to the collision detection management software.\\n        Your callback will be activated when a collision occurs.\\n\\n    This software is kept separate from the worldServer applications because\\n    the application developer wants to build a library of extended objects\\n    from which to choose.\\n\\n    The following is all you need to make a World Server application:\\n\\n    o Provide an initWorld function:\\n        This is where you choose what object extensions will be supported, plus\\n        any initialization you want to do.\\n\\n    o Provide a positionObject function:\\n        This is where you determine where to place a new client.\\n\\n    o Provide an installWorldObjects function:\\n        This is where you load the world (.wld) file for a new client.\\n\\n    o Provide a getWorldType function:\\n        This is where you tell a new client what persona they should have.\\n\\n    o Provide an animateWorld function:\\n        This is where you can go wild! At a minimum you should let the objects\\n        move (by calling a move function) and let the server sleep for a bit\\n        (to avoid outrunning the clients).\\n\\n    That's all there is to it! And to prove it here are the line counts for the\\n    three world servers I've provided:\\n\\n        generic - 81 lines\\n        dactyl - 270 lines (more complicated collision detection due to the\\n                           stairs! Will probably be improved with future\\n                           versions)\\n        dogfight - 72 lines\\n\\nLocation:\\n\\n   This software is located at the following site:\\n   ftp.u.washington.edu\\n\\n   Directory:\\n   pub/virtual-worlds\\n\\n   File:\\n   multiverse-1.0.2.tar.Z\\n\\nFutures:\\n\\n   Client:\\n\\n    o Texture mapping.\\n\\n    o More realistic rendering: i.e. Z-Buffering (or similar), Gouraud shading\\n\\n    o HMD support.\\n\\n    o Etc, etc....\\n\\n   Server:\\n\\n    o Physical Modelling (gravity, friction etc).\\n\\n    o Enhanced Object Management/Interaction\\n\\n    o Etc, etc....\\n\\n   Both:\\n\\n    o Improved Comms!!!\\n\\nI hope this provides people with a good understanding of the Multiverse\\nsoftware,\\nunfortunately it comes with practically zero documentation, and I'm not sure\\nwhether that will ever be able to be rectified! :-(\\n\\nI hope people enjoy this software and that it is useful in our explorations of\\nthe Virtual Universe - I've certainly found fascinating developing it, and I\\nwould *LOVE* to add support for the PowerGlove...and an HMD :-)!!\\n\\nFinally one major disclaimer:\\n\\nThis is totally amateur code. By that I mean there is no support for this code\\nother than what I, out the kindness of my heart, or you, out of pure\\ndesperation, provide. I cannot be held responsible for anything good or bad\\nthat may happen through the use of this code - USE IT AT YOUR OWN RISK!\\n\\nDisclaimer over!\\n\\nOf course if you love it, I would like to here from you. And anyone with\\nPOSITIVE contributions/criticisms is also encouraged to contact me. Anyone who\\nhates it: > /dev/null!\\n\\n************************************************************************\\n*********\\nAnd if anyone wants to let me do this for a living: you know where to\\nwrite :-)!\\n************************************************************************\\n*********\\n\\nThanks,\\n\\nRobert.\\n\\nrobert@acsc.com\\n^^^^^^^^^^^^^^^\\n\"\n \"From: lulagos@cipres.cec.uchile.cl (admirador)\\nSubject: OAK VGA 1Mb. Please, I needd VESA TSR!!! 8^)\\nOriginator: lulagos@cipres\\nNntp-Posting-Host: cipres.cec.uchile.cl\\nOrganization: Centro de Computacion (CEC), Universidad de Chile\\nLines: 15\\n\\n\\n\\tHi there!...\\n\\t\\tWell, i have a 386/40 with SVGA 1Mb. (OAK chip 077) and i don't\\n\\t\\thave VESA TSR program for this card. I need it . \\n\\t\\t\\tPlease... if anybody can help me, mail me at:\\n\\t\\t\\tlulagos@araucaria.cec.uchile.cl\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThanks.\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMackk. \\n   _   /| \\n   \\\\'o.O' \\n   =(___)=\\n      U   \\n     Ack!\\n\"\n 'From: wcs@anchor.ho.att.com (Bill Stewart +1-908-949-0705)\\nSubject: Re: Clipper considered harmful\\nOrganization: Sorcerer\\'s Apprentice Cleaning Services\\nDistribution: inet\\nIn-Reply-To: shirriff@sprite.berkeley.edu\\'s message of 21 Apr 1993 00:36:44 GMT\\nNntp-Posting-Host: rainier.ho.att.com\\nLines: 24\\n\\nIn article <1r24us$oeh@agate.berkeley.edu> shirriff@sprite.berkeley.edu (Ken Shirriff) writes:\\n   In article <15469@optilink.COM> brad@optilink.COM (Brad Yearwood) writes:\\n   >Finally, because there is essentially no possibility of intercepting in\\n   >realtime the scrutable content of communications between stolen instruments,\\n   >there will exist strong motivation to record and archive _all_ communications\\n   >in the network for ex-post-facto scrutiny (once some criminal act is\\n   >discovered, and the instruments involved have been identified).\\n\\n\"All\" is a *very* big number.  The AT&T Long Distance network has\\naround 20,000 T3 trunks (45 Mbit/sec), which is on the order of 10**12 bits/sec.\\nThat doesn\\'t even count the amount of traffic in the LOCAL phone companies,\\nor our long-distance competitors.  It\\'s about 200 Exabytes tapes / second,\\nwhich is pretty large even for the NSA :-)\\n\\nOn the other hand, I can easily see them recording the traffic for\\n\"interesting\" people, such as dissidents, suspected criminals,\\nforeign telephone calls, and anybody noticed using encryption.\\nAs Ken Shiriff speculates, recording encrypted traffic will probably\\nbe judged not to be an invasion of privacy pretty soon ....\\n--\\n#\\t\\t\\t\\tPray for peace;      Bill\\n# Bill Stewart 1-908-949-0705 wcs@anchor.att.com AT&T Bell Labs 4M312 Holmdel NJ\\n#\\t              No, I\\'m *from* New Jersey, I only *work* in cyberspace....\\n# White House Commect Line 1-202-456-1111  fax 1-202-456-2461\\n'\n ...\n \"From: gtoal@gtoal.com (Graham Toal)\\nSubject: Re: PGP ideas for IBM systems\\nLines: 7\\n\\n:    I've been thinking about how difficult it would be to make PGP available\\n: in some form on EBCDIC machines.\\n\\nDon't encourage them.  Let EBCDIC machines die an honorable death :)\\n\\nG\\n\\n\"\n \"From: doug@hparc0.aus.hp.com (Doug Parsons)\\nSubject: Re: 3d-Studio V2.01 : Any differences with previous version\\nOrganization: HP Australasian Response Centre (Melbourne)\\nX-Newsreader: TIN [version 1.1 PL8.5]\\nLines: 10\\n\\nFOMBARON marc (fombaron@ufrima.imag.fr) wrote:\\n: Are there significant differences between V2.01 and V2.00 ?\\n: Thank you for helping\\n\\n\\nNo.  As I recall, the only differences are in the 3ds.set parameters - some\\nof the defaults have changed slightly.  I'll look when I get home and let\\nyou know, but there isn't enough to actually warrant upgrading.\\n\\ndouginoz\\n\"\n 'From: rboudrie@chpc.org (Rob Boudrie)\\nSubject: Re: Once tapped, your code is no good any more.\\nOrganization: Center For High Perf. Computing of WPI; Marlboro Ma\\nDistribution: na\\nLines: 26\\n\\nIn article <C5so84.Hxv@demon.co.uk> Graham Toal <gtoal@gtoal.com> writes:\\n>In article <2073@rwing.UUCP> pat@rwing.UUCP (Pat Myrto) writes:\\n>:If the Clinton Clipper is so very good, why not make its algrithm public\\n>:so many people can exchange ideas and examine it, rather than a few\\n>:isolated \\'respected experts\\' (respected by whom?  for what?  Perhaps a\\n\\nOne more time...\\n\\n    If they released the algorithm, it would be possible for someone\\n    to come up with an implementation which was identical, but\\n    lacking an escrowed key.\\n\\n    Note that the press announcement mentioned that the algorithm was\\n    being kept secret for security of the key escrow system.  In this\\n    case security means \"an escrowed key for EVERY clipper chip\".\\n\\n\\n    Assuming you believed all that is said about the effective of\\n    the algorithm, and the escrow system, which would you buy :\\n\\n     (a)  Chip from firm A with the escrowed key\\n     (b)  Second source chip from reputable firm B with no key\\n          in government escrow.\\n\\n    There would obviously be powerful economic incentives for a second\\n    source, non escrowed, vendor.\\n'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "tfidf = text.TfidfTransformer()\n",
    "tfidf.fit_transform(ng_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320838f",
   "metadata": {},
   "source": [
    "We can to figure out why, we can compare what that method takes as input to what the count vectorizer takes as input. While working, the easiest way to do this is using the jupyter help (shift+tab, or ?) but for the notes, I'll print them out differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b2d6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the idf vector (global term weights).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : sparse matrix of shape n_samples, n_features)\n",
      "            A matrix of term/token counts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(tfidf.fit.__doc__.split('\\n')[:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05549cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        raw_documents : iterable\n",
      "            An iterable which generates either str, unicode or file objects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(count_vec.fit.__doc__.split('\\n')[:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6959d",
   "metadata": {},
   "source": [
    "We wanted the TfidfVectorizer, not the transformer, so that it accepts documents not features. We will again, instantiate the object and then fit on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe7a3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = text.TfidfVectorizer()\n",
    "\n",
    "tfidf.fit(ng_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d878849",
   "metadata": {},
   "source": [
    "We can see this works, because the code runs, but for completeness , we can also check the input again to compare with the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882179ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn vocabulary and idf from training set.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        raw_documents : iterable\n",
      "            An iterable which generates either str, unicode or file objects.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(tfidf.fit.__doc__.split('\\n')[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c128aa",
   "metadata": {},
   "source": [
    "This now takes documents as we wanted. Since we split the data before transforming, we can then apply the new fit using the transform on the train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "303e1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_tfidf_train = tfidf.transform(ng_X_train)\n",
    "ng_tfidf_test = tfidf.transform(ng_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76daf18e",
   "metadata": {},
   "source": [
    "Since these splits were made before we can use the same targets we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0890ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9288135593220339"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(ng_tfidf_train,ng_y_train).score(ng_tfidf_test,ng_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b6af8",
   "metadata": {},
   "source": [
    "## Comparing representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e09426",
   "metadata": {},
   "source": [
    "To start, we will look at one element from each in order to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc4b86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24257 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 84 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_tfidf_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86a68d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24257 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 84 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_vec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc8d98",
   "metadata": {},
   "source": [
    "To start they both have 84 elements, since it is two different representations of the same document, that makes sense.  We can check a few others as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea8fcaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24257 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 202 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_tfidf_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddb18956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24257 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 202 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_vec_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "553673d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ng_vec_train[4]>0).sum() == (ng_tfidf_train[4]>0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0489e",
   "metadata": {},
   "source": [
    "Let's pick out a common word so that the calculation is meaningful and do the tfidf calucation. To find a common word in the dictionary, we'll first filter the vocabulary to keep only the words that occur at least 300 times in the training set. We sum along the columns of the matrix, transform it to an array, then iterate over the sum, enumerated (assigning the number to each element of the sum) and use that to get the word out, if its total is over 300.  I saw that this is actually a sort of long list, so I chose to only print out the first 25. We print them out with the index so we can use it for the one we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ad7ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('about', 3326),\n",
       " ('all', 3819),\n",
       " ('also', 3874),\n",
       " ('an', 3964),\n",
       " ('and', 4003),\n",
       " ('any', 4115),\n",
       " ('are', 4263),\n",
       " ('article', 4347),\n",
       " ('as', 4363),\n",
       " ('at', 4456),\n",
       " ('available', 4612),\n",
       " ('be', 4851),\n",
       " ('been', 4887),\n",
       " ('bit', 5096),\n",
       " ('but', 5582),\n",
       " ('by', 5605),\n",
       " ('can', 5779),\n",
       " ('chip', 6191),\n",
       " ('clipper', 6393),\n",
       " ('com', 6568),\n",
       " ('computer', 6786),\n",
       " ('could', 7180),\n",
       " ('cs', 7426),\n",
       " ('data', 7687),\n",
       " ('db', 7730)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(count_vec.get_feature_names()[i],i) for i, n in \n",
    "         enumerate(np.asarray(ng_vec_train.sum(axis=0))[0])\n",
    " if n>300][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32d09c",
   "metadata": {},
   "source": [
    "Let's use computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f975d051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computer_idx = 6786\n",
    "count_vec.get_feature_names()[computer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65615a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 5, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_vec_train[:,computer_idx].toarray()[:10].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe009eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.06907742, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_tfidf_train[:,computer_idx].toarray()[:10].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbed061",
   "metadata": {},
   "source": [
    "So, we can see they have non zero elements in the same places, meaning that in both representations the column refers to the same thing. \n",
    "\n",
    "We can compare the untransformed to the count vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd33de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ng_X_train[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4791326d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_vec_train[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46db317",
   "metadata": {},
   "source": [
    "We see that it is just a count of the number of total words, not unique words, but total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e5de364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.558255873996122"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_tfidf_train[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4de2e",
   "metadata": {},
   "source": [
    "The tf-idf matrix, however is normalized to make the sums smaller. Each row is not the same, but it is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "150faff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fb8a2d30910>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 402.25x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/home/runner/work/BrownFall21/BrownFall21/_build/jupyter_execute/notes/2021-11-29_49_1.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(ng_vec_train.sum(axis=1),bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8c2e924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fb8a186dcd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 402.25x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/home/runner/work/BrownFall21/BrownFall21/_build/jupyter_execute/notes/2021-11-29_50_1.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(ng_tfidf_train.sum(axis=1),bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3fd2ca",
   "metadata": {},
   "source": [
    "We can see that the `tf-idf` makes the totals across documents more spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64eea6",
   "metadata": {},
   "source": [
    "When we sum across words, we then get to see how"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d92c1a",
   "metadata": {},
   "source": [
    "From the documentation we see that the idf is not exactly the inverse of the number of documents, it's also rescaled some. \n",
    "\n",
    "$$ idf = \\log \\frac{1 +n }{1 +df} + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007264c1",
   "metadata": {},
   "source": [
    "In this implementation, each row is the normalized as well, to keep them small, so that documents of different sizes are more comparable.  For example, if we return to what we did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f80385f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load http://drsmb.co/310read\n",
    "\n",
    "# add an entry to the following dictionary with a name as the key and a sentence as the value\n",
    "# share a sentence about how you're doing this week\n",
    "# remember this will be python code, don't use\n",
    "# You can remain anonymous (this page & the notes will be fully public)\n",
    "# by attributing it to a celebrity or psuedonym, but include *some* sort of attribution\n",
    "sentence_dict = {\n",
    "'Professor Brown':\"I'm excited for Thanksgiving.\",\n",
    "'Matt Langton':\"I'm doing pretty good, I'll be taking the days off to catch up on various classwork.\",\n",
    "'Evan':\"I'm just here so my grade doesn't get fined\",\n",
    "'Greg Bassett':\"I'm doing well, my birthday is today. I'm looking forward to seeing my family this Thursday, I haven't seen a lot of them in a long time.\",\n",
    "'Noah N':\"I'm doing well! I can't wait to take opportuity of this long weekend to catch up on various HW's, projects, etc.\",\n",
    "'Tuyetlinh':\"I'm struggling to get all my work done before break, but I'm excited to have that time off when I'm all done.\",\n",
    "'Kenza Bouabdallah':\"I am doing good. How are you ?\",\n",
    "'Chris Kerfoot':\"I'm doing pretty good. I'm happy to have some days off this week because of Thanksgiving!\",\n",
    "'Kang Liu': \"New week, new start\",\n",
    "'Aiden Hill':\"I am very much enjoying this class.\",\n",
    "'Muhammad S':\"I am doing pretty well. I am looking forward to taking a few days off.\",\n",
    "'Max Mastrorocco':\"Cannot wait for a break.\",\n",
    "'Daniel':\"I am doing well. I am ready and excited for break!\",\n",
    "'Nate':\"I'm just vibing right now, ready for break \",\n",
    "'Jacob':\"I am going to eat Turkey.\",\n",
    "'Anon':\"nom nom nom\"\n",
    "}\n",
    "# create a new count vecorizer so that we can return to the news analysis\n",
    "count_vec_ex = text.CountVectorizer()\n",
    "tfidf_ex = text.TfidfVectorizer()\n",
    "mat_c = count_vec_ex.fit_transform(sentence_dict.values()).toarray()\n",
    "mat_t = tfidf_ex.fit_transform(sentence_dict.values()).toarray()\n",
    "# sentence_df = pd.DataFrame(data = mat_c, \n",
    "#                            columns =counts.get_feature_names_out(),\n",
    "#                           index=sentence_dict.keys())\n",
    "\n",
    "dist_df_count = pd.DataFrame(data = euclidean_distances(mat_c),\n",
    "            index= sentence_dict.keys(), columns= sentence_dict.keys())\n",
    "dist_df_tfidf = pd.DataFrame(data = euclidean_distances(mat_t),\n",
    "            index= sentence_dict.keys(), columns= sentence_dict.keys())\n",
    "\n",
    "ref = 'Professor Brown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47a106f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Max Mastrorocco'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_closest = dist_df_count[ref].drop(ref).idxmin()\n",
    "count_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4523f751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniel'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_closest = dist_df_tfidf[ref].drop(ref).idxmin()\n",
    "tfidf_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6ca08d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm excited for Thanksgiving.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_dict[ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8eb09a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cannot wait for a break.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_dict[count_closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "189e0c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am doing well. I am ready and excited for break!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_dict[tfidf_closest]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd8bbe",
   "metadata": {},
   "source": [
    "Now, the closest sentence actually shares a wrod that's similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7144dbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Max Mastrorocco      2.236068\n",
       "Nate                 2.828427\n",
       "Jacob                2.828427\n",
       "Kenza Bouabdallah    3.000000\n",
       "Kang Liu             3.000000\n",
       "Aiden Hill           3.000000\n",
       "Daniel               3.162278\n",
       "Evan                 3.316625\n",
       "Anon                 3.464102\n",
       "Chris Kerfoot        3.872983\n",
       "Muhammad S           4.123106\n",
       "Matt Langton         4.242641\n",
       "Noah N               4.898979\n",
       "Tuyetlinh            5.099020\n",
       "Greg Bassett         5.196152\n",
       "Name: Professor Brown, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df_count[ref].drop(ref).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b0fce4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Daniel               1.153404\n",
       "Max Mastrorocco      1.248303\n",
       "Chris Kerfoot        1.279299\n",
       "Nate                 1.299060\n",
       "Tuyetlinh            1.341626\n",
       "Noah N               1.414214\n",
       "Kenza Bouabdallah    1.414214\n",
       "Aiden Hill           1.414214\n",
       "Muhammad S           1.414214\n",
       "Jacob                1.414214\n",
       "Anon                 1.414214\n",
       "Matt Langton         1.414214\n",
       "Evan                 1.414214\n",
       "Greg Bassett         1.414214\n",
       "Kang Liu             1.414214\n",
       "Name: Professor Brown, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df_tfidf[ref].drop(ref).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d7a1f",
   "metadata": {},
   "source": [
    "Now, the distances are all much smaller and Chris who also talked about Thanksgiving is much higher on the list too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b0147d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 15,  8, 22, 19, 19,  6, 14,  4,  6, 12,  4,  9,  7,  5,  3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_c.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69f569b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.72586983, 3.81967092, 2.81780765, 4.4761216 , 4.1953457 ,\n",
       "       3.75971487, 2.39375957, 3.68712845, 1.61308516, 2.41468677,\n",
       "       3.22271608, 1.97816206, 2.73725829, 2.62109557, 2.17780895,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_t.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69670188",
   "metadata": {},
   "source": [
    "The sums are again much closer so the total length isn't as much of the signal. Since it's smaller, we can also sum along words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37155f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sent_word_df = pd.DataFrame(data = count_vec_ex.get_feature_names(),\n",
    "                           columns = ['word'])\n",
    "sent_word_df['count_tot'] = mat_c.sum(axis=0)\n",
    "sent_word_df['tfidf_tot'] = mat_t.sum(axis=0)\n",
    "sent_word_df['doc_count'] = (mat_c>0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af0b085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count_tot</th>\n",
       "      <th>tfidf_tot</th>\n",
       "      <th>doc_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>to</td>\n",
       "      <td>9</td>\n",
       "      <td>1.513881</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doing</td>\n",
       "      <td>7</td>\n",
       "      <td>1.326396</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am</td>\n",
       "      <td>7</td>\n",
       "      <td>1.943291</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>off</td>\n",
       "      <td>4</td>\n",
       "      <td>0.860791</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>this</td>\n",
       "      <td>4</td>\n",
       "      <td>0.899345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>happy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>grade</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>going</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fined</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>you</td>\n",
       "      <td>1</td>\n",
       "      <td>0.479913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  count_tot  tfidf_tot  doc_count\n",
       "73     to          9   1.513881          7\n",
       "17  doing          7   1.326396          7\n",
       "1      am          7   1.943291          5\n",
       "51    off          4   0.860791          4\n",
       "70   this          4   0.899345          4\n",
       "..    ...        ...        ...        ...\n",
       "32  happy          1   0.327365          1\n",
       "31  grade          1   0.374658          1\n",
       "29  going          1   0.517461          1\n",
       "25  fined          1   0.374658          1\n",
       "86    you          1   0.479913          1\n",
       "\n",
       "[87 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_word_df.sort_values(by='doc_count',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74718383",
   "metadata": {},
   "source": [
    "## Sparse Matrices\n",
    "\n",
    "To understand the sparse matrices, we can examine the size of the objects as a sparse matrix and after exporting to array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "032d3de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(ng_vec_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "461d95b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194176"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(ng_vec_train[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a1ca7",
   "metadata": {},
   "source": [
    "This is another reason it is better to use the sparse matrices (as returned by `transform`, but not by `fit_transform`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91380e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_tfidf_train[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9907e7",
   "metadata": {},
   "source": [
    "## More Practice\n",
    "\n",
    "1. On the sentence dataset, try implementing the tf-idf calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c99ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "source_map": [
   12,
   16,
   29,
   41,
   52,
   68,
   72,
   76,
   78,
   82,
   86,
   94,
   97,
   103,
   108,
   112,
   116,
   120,
   123,
   127,
   131,
   133,
   137,
   141,
   145,
   147,
   151,
   154,
   158,
   160,
   164,
   168,
   172,
   174,
   178,
   182,
   186,
   188,
   192,
   196,
   200,
   205,
   209,
   211,
   217,
   221,
   223,
   227,
   229,
   233,
   237,
   239,
   243,
   247,
   253,
   257,
   300,
   305,
   310,
   314,
   318,
   320,
   324,
   328,
   330,
   334,
   338,
   340,
   344,
   352,
   354,
   361,
   365,
   367,
   371,
   373,
   379
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}