---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Interpretting Regression
```{code-cell} ipython3
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import itertools as itr
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
sns.set_theme(font_scale=2,palette='colorblind')
```
we'll return to the same data we used on Monday, first.

```{code-cell} ipython3
tips = sns.load_dataset("tips").dropna()
```

```{code-cell} ipython3
tips.shape
```

```{code-cell} ipython3
tips.head()
```

Again, we'll prepare the data.

```{code-cell} ipython3

# sklearn requires 2D object of features even for 1 feature
tips_X = tips['total_bill'].values
tips_X = tips_X[:,np.newaxis] # add an axis
tips_y = tips['tip']

tips_X_train,tips_X_test, tips_y_train, tips_y_test = train_test_split(
                     tips_X,
                     tips_y,
                     train_size=.8,
                     random_state=0)
```

Next, we'll fit the model

```{code-cell} ipython3
regr_tips = linear_model.LinearRegression()
regr_tips.fit(tips_X_train,tips_y_train)
regr_tips.score(tips_X_test,tips_y_test)
```


This doesn't perform all that well, but let's investigate it further.
We'll start by looking at the residuals

```{code-cell} ipython3
tips_y_pred = regr_tips.predict(tips_X_test)
```
## Examining Residuals

The error, the difference between the predictions and the truth is called the
residual.

```{code-cell} ipython3
tips_y_pred - tips_y_test
```

To examine these, we can plot them on the data:
```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')

[plt.plot([x,x],[yp,yt], color='red', linewidth=3)
         for x, yp, yt in zip(tips_X_test, tips_y_pred,tips_y_test)];
```

We can plot them as a scatter plot as well.
```{code-cell} ipython3

tips_residuals = tips_y_pred - tips_y_test
plt.scatter(tips_X_test,tips_residuals, color='red')
```

One thing we notice is that the residuals are smaller for some values of the
`total_bill`  and larger for others. This suggests that there is more
information left.
A good fit, would have residuals that are evenly distributed, not correlated
with the feature(s) in this case, the total bill.


## Polynomial regression

Polynomial regression is still a linear problem.  Linear regression solves for
the $\beta_i$ for a $d$ dimensional problem.

$$ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_d x_d = \sum_i^d \beta_i x_i$$

Quadratic regression solves for

$$ y = \beta_0 + \sum_i^d \beta_i x_i$ + \sum_j^d \sum_i^d \beta_{d+i} x_i x_j + \sum_i^d x_i^2$ $$

This is still a linear problem, we can create a new $X$ matrix that has the
polynomial values of each feature  and solve for more $\beta$ values.

We use a transformer object, which works similarly to the estimators, but does
not use targets.
First, we instantiate.

```{code-cell} ipython3
poly = PolynomialFeatures(include_bias=False)
```

Then we apply it
```{code-cell} ipython3
tips_X2_train = poly.fit_transform(tips_X_train)
tips_X2_test = poly.fit_transform(tips_X_test)
```


We can see wht it did by looking at the shape.
```{code-cell} ipython3
tips_X_train.shape, tips_X2_train.shape
```

```{code-cell} ipython3
tips_X2_train[:5,1]
```

```{code-cell} ipython3
tips_X_train[:5]
```

```{code-cell} ipython3
tips_X2_train[:5,0]
```

Now, we can fit a linear model on this data, which learns a weight for the data
and it's squared value.

```{code-cell} ipython3
regr2_tips = linear_model.LinearRegression()
regr2_tips.fit(tips_X2_train,tips_y_train)
tips2_y_pred = regr2_tips.predict(tips_X2_test)
```
Then we can plot it.

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
plt.scatter(tips_X_test,tips2_y_pred, color='green')
```
We can see that this its somewhat better, the residuals are more uniformly
distributed, but it doesn't look very nonlinear.  
We will examine this further in the next step, but first we will drop the linear column to see the quadratic more clearly.


```{code-cell} ipython3
poly = PolynomialFeatures()

tips_Xq_train = poly.fit_transform(tips_X_train)[:,::2]
tips_Xq_test = poly.fit_transform(tips_X_test)[:,::2]

regr_qu_tips = linear_model.LinearRegression(fit_intercept=False)
regr_qu_tips.fit(tips_Xq_train,tips_y_train)
tips2_q_pred = regr_qu_tips.predict(tips_Xq_test)
```


```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips2_q_pred, color='green')
```

```{admonition} Try it Yourself
How would you make it cubic? what about 4th dimension?
```

## Examining Coefficients


Now we can compare the coefficients.
We saw above that the quadratic didn't help much, so let's look at those.

```{code-cell} ipython3
regr2_tips.coef_
```
The second parameter is very very small, so that explains why it didn't change
the fit much. We can use the features to figure out how important each
feature is to the prediction. Large numbers strongly influence the prediction
smaller ones influence it less.

```{code-cell} ipython3
regr_tips.coef_
```

<!-- ## Multiple variables


```{admonition} Thinking Ahead
If this is not fully clear, it's okay.
Using multiple variables or nonlinear regression is an extension you can use for
your portfolio to earn level 3.  

```
We can also transform the other features to numerical features and then use that larger feature vector.

This binarizes the varialbes.
```{code-cell} ipython3
bin_cols = ['total_bill','sex','smoker','time']
tips_bin = tips[bin_cols].replace({'Female':1,'Male':0,
                                   'No':0,'Yes':1,
                                  'Dinner':1,'Lunch':0})
tips_bin.shape
```
This gets dummies for the other variables and concats this

```{code-cell} ipython3

tips_onehot = pd.concat([tips['total_bill'],
                         pd.get_dummies(tips['day']),
                        pd.get_dummies(tips['time'])],axis=1)

tips_onehot.head()
```

```{code-cell} ipython3
tips_onehot
```

```{code-cell} ipython3
col_names = (list(tips_onehot.columns) +
[a+'_'+b + '_' +c +'_'+d for a,b,c,d in itr.combinations(tips_onehot.columns,4)])
```

```{code-cell} ipython3
col_names[11:90]
```

```{code-cell} ipython3
sns.lmplot(data=tips,x='total_bill',y='tip',
          col='sex',row='smoker',hue='time')
```

```{code-cell} ipython3
interaction = PolynomialFeatures(interaction_only=True,
                                include_bias=False)
tips_interaction = interaction.fit_transform(tips_onehot,)
tips_all_X_train,tips_all_X_test, tips_all_y_train, tips_all_y_test = train_test_split(
                     tips_interaction,
                     tips_y,
                     train_size=.8,
                     random_state=0)
```

```{code-cell} ipython3
tips_all_X_train.shape
```

```{code-cell} ipython3
tips_all_X_train
```

```{code-cell} ipython3
regr_all_tips = linear_model.LinearRegression()
regr_all_tips.fit(tips_all_X_train,tips_all_y_train)
tips_all_y_pred = regr_all_tips.predict(tips_all_X_test,)
regr_all_tips.score(tips_all_X_test,tips_all_y_test)
```

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
plt.scatter(tips_X_test,tips_all_y_pred, color='green')
```

```{code-cell} ipython3
regr_all_tips.coef_
``` -->

## Sparse Regression

An extreme is for some coefficients to be zero.
The LASSO model, constrains some of the coefficients to be 0, so it learns
simultanesouly how to combine the features to predict the target and which
subset of the features to use.

```{admonition} Further Reading
For the mathermatical formulation see the sklearn [User Guide Section on LASSO](https://scikit-learn.org/stable/modules/linear_model.html#lasso)
and the code in [LASSO docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)
```

```{admonition} Thinking Ahead
LASSO is not required for assignment 8, but is one way you could earn level 3.
Here is a preview, but you can investigate it further on your own.
```

```{code-cell} ipython3
tips_lasso = linear_model.Lasso(alpha=.0025)
tips_lasso.fit(tips_all_X_train,tips_all_y_train)
tips_lasso_y_pred = tips_lasso.predict(tips_all_X_test,)
tips_lasso.score(tips_all_X_test,tips_all_y_test)
```

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
plt.scatter(tips_X_test,tips_lasso_y_pred, color='green')
```

```{code-cell} ipython3
sum(tips_lasso.coef_ ==0)/len(tips_lasso.coef_)
```

```{code-cell} ipython3
tips_onehot.shape, tips_interacion.shape
```

The transform changed our data from 10 columns to 55.

```{code-cell} ipython3
tips_interacion.head
```

<!-- work through dummies

+++

show lasso

```{code-cell} ipython3

``` -->

## Questions After Class


### When do we do regression?
```{toggle}
we do regresion, when we want to predict a continuous value
```


### What should I look for in datasets to know whether a linear model or non-linear model is best?
```{toggle}
If you know a reason to choose one from domain knowledge, always use that. From
data alone, a reasonable thing to do is to fit a linear model and then examine
the residuals and use a more complex model if that makes sense.
```

### How can we tell if a dataset is going to be useful through tweaking or is just not worth it?
```{toggle}
This is a **very** good question, but does not have a simple answer. In some
cases, a moderate fit quality is enough, because there's low risk of making
errors.  In other cases, a really high quality fit is required because of the
risk.
```
