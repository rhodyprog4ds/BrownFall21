---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# More NLP & Solving problems with ML

+++

Imagine you work for a news agency, current articles are stored in a database in
a table where they are indexed by a unique identifier. A separate table indicates
the primary category for most articles, but some are missing. Your assignment is to create an automatic category generator to save editors time. Your manager suggests that
an SVM or decision tree is probably best, but is unsure what text representations
will perform best. You are expected to produce an accessible report with tables and plots that visualize the performance and impact of various modeling choices. 



Write the import statements you would need to solve this problem. Include
whole libraries or modules as is most appropriate.

+++

```python 
import pandas as pd
import sqlite3
from sklearn.feature_extraction import text
from sklearn import tree
from sklearn import svm
from sklearn import model_selection
import seaborn as sns
```

+++

Given the following vocabulary,

['and', 'are', 'cat', 'cats', 'dogs', 'pets', 'popular', 'videos']


represent "Cats and dogs are pets" in vector format.

[1,1,0,1,1,1,0,0]

```{code-cell} ipython3
from sklearn.feature_extraction import text
from sklearn.metrics.pairwise import euclidean_distances
from sklearn import datasets
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# import a python utility for examingin ojbects for the notes
from sys import getsizeof
import numpy as np
import seaborn as sns
ng_X,ng_y = datasets.fetch_20newsgroups(categories =['comp.graphics','sci.crypt'],
                    return_X_y = True)
```

```{code-cell} ipython3
type(ng_y)
```

```{code-cell} ipython3
ng_y[:5]
```

```{code-cell} ipython3
ng_X[0]
```

We're going to instantiate the object and fit it two the whole dataset.

```{code-cell} ipython3
count_vec = text.CountVectorizer()

count_vec.fit(ng_X)
```

```{important}
I changed the following a little bit from class so that we can use the same test/train split for the two different types of transformation so that we can compare them more easily. 

This also helps illustrate when using the fit and transform separately is helpful. 
```

```{code-cell} ipython3
ng_X_train, ng_X_test, ng_y_train, ng_y_test = train_test_split(
                                        ng_X, ng_y, random_state=0)
```

Now, we can use the transformation that we fit to the whole dataset to transform the train and test portions of the data separately. 

The transform method also returns the sparse matrix directly so we no longer need the `toarray` method. 

```{code-cell} ipython3
ng_vec_train = count_vec.transform(ng_X_train)
ng_vec_test = count_vec.transform(ng_X_test)
```

```{code-cell} ipython3
ng_vec_train[0]
```

```{code-cell} ipython3
clf = MultinomialNB()
```

```{code-cell} ipython3
clf.fit(ng_vec_train,ng_y_train).score(ng_vec_test,ng_y_test)
```

```{code-cell} ipython3
tfidf = text.TfidfTransformer()
tfidf.fit_transform(ng_X)
```

We can to figure out why, we can compare what that method takes as input to what the count vectorizer takes as input. While working, the easiest way to do this is using the jupyter help (shift+tab, or ?) but for the notes, I'll print them out differently.

```{code-cell} ipython3
print('\n'.join(tfidf.fit.__doc__.split('\n')[:7]))
```

```{code-cell} ipython3
print('\n'.join(count_vec.fit.__doc__.split('\n')[:7]))
```

We wanted the TfidfVectorizer, not the transformer, so that it accepts documents not features. We will again, instantiate the object and then fit on the whole dataset. 

```{code-cell} ipython3
tfidf = text.TfidfVectorizer()

tfidf.fit(ng_X)
```

We can see this works, because the code runs, but for completeness , we can also check the input again to compare with the above. 

```{code-cell} ipython3
print('\n'.join(tfidf.fit.__doc__.split('\n')[:6]))
```

This now takes documents as we wanted. Since we split the data before transforming, we can then apply the new fit using the transform on the train/test splits. 

```{code-cell} ipython3
ng_tfidf_train = tfidf.transform(ng_X_train)
ng_tfidf_test = tfidf.transform(ng_X_test)
```

Since these splits were made before we can use the same targets we used above. 

```{code-cell} ipython3
clf.fit(ng_tfidf_train,ng_y_train).score(ng_tfidf_test,ng_y_test)
```

## Comparing representations

+++

To start, we will look at one element from each in order to compare them. 

```{code-cell} ipython3
ng_tfidf_train[0]
```

```{code-cell} ipython3
ng_vec_train[0]
```

To start they both have 84 elements, since it is two different representations of the same document, that makes sense.  We can check a few others as well

```{code-cell} ipython3
ng_tfidf_train[1]
```

```{code-cell} ipython3
ng_vec_train[1]
```

```{code-cell} ipython3
(ng_vec_train[4]>0).sum() == (ng_tfidf_train[4]>0).sum()
```

Let's pick out a common word so that the calculation is meaningful and do the tfidf calucation. To find a common word in the dictionary, we'll first filter the vocabulary to keep only the words that occur at least 300 times in the training set. We sum along the columns of the matrix, transform it to an array, then iterate over the sum, enumerated (assigning the number to each element of the sum) and use that to get the word out, if its total is over 300.  I saw that this is actually a sort of long list, so I chose to only print out the first 25. We print them out with the index so we can use it for the one we choose.  

```{code-cell} ipython3
[(count_vec.get_feature_names()[i],i) for i, n in 
         enumerate(np.asarray(ng_vec_train.sum(axis=0))[0])
 if n>300][:25]
```

Let's use computer.

```{code-cell} ipython3
computer_idx = 6786
count_vec.get_feature_names()[computer_idx]
```

```{code-cell} ipython3
ng_vec_train[:,computer_idx].toarray()[:10].T
```

```{code-cell} ipython3
ng_tfidf_train[:,computer_idx].toarray()[:10].T
```

So, we can see they have non zero elements in the same places, meaning that in both representations the column refers to the same thing. 

We can compare the untransformed to the count vectorizer:

```{code-cell} ipython3
len(ng_X_train[0].split())
```

```{code-cell} ipython3
ng_vec_train[0].sum()
```

We see that it is just a count of the number of total words, not unique words, but total

```{code-cell} ipython3
ng_tfidf_train[0].sum()
```

The tf-idf matrix, however is normalized to make the sums smaller. Each row is not the same, but it is more similar. 

```{code-cell} ipython3
sns.displot(ng_vec_train.sum(axis=1),bins=20)
```

```{code-cell} ipython3
sns.displot(ng_tfidf_train.sum(axis=1),bins=20)
```

We can see that the `tf-idf` makes the totals across documents more spread out. 

+++

When we sum across words, we then get to see how 

+++

From the documentation we see that the idf is not exactly the inverse of the number of documents, it's also rescaled some. 

$$ idf = \log \frac{1 +n }{1 +df} + 1 $$

+++

In this implementation, each row is the normalized as well, to keep them small, so that documents of different sizes are more comparable.  For example, if we return to what we did last week. 

```{code-cell} ipython3
# %load http://drsmb.co/310read

# add an entry to the following dictionary with a name as the key and a sentence as the value
# share a sentence about how you're doing this week
# remember this will be python code, don't use
# You can remain anonymous (this page & the notes will be fully public)
# by attributing it to a celebrity or psuedonym, but include *some* sort of attribution
sentence_dict = {
'Professor Brown':"I'm excited for Thanksgiving.",
'Matt Langton':"I'm doing pretty good, I'll be taking the days off to catch up on various classwork.",
'Evan':"I'm just here so my grade doesn't get fined",
'Greg Bassett':"I'm doing well, my birthday is today. I'm looking forward to seeing my family this Thursday, I haven't seen a lot of them in a long time.",
'Noah N':"I'm doing well! I can't wait to take opportuity of this long weekend to catch up on various HW's, projects, etc.",
'Tuyetlinh':"I'm struggling to get all my work done before break, but I'm excited to have that time off when I'm all done.",
'Kenza Bouabdallah':"I am doing good. How are you ?",
'Chris Kerfoot':"I'm doing pretty good. I'm happy to have some days off this week because of Thanksgiving!",
'Kang Liu': "New week, new start",
'Aiden Hill':"I am very much enjoying this class.",
'Muhammad S':"I am doing pretty well. I am looking forward to taking a few days off.",
'Max Mastrorocco':"Cannot wait for a break.",
'Daniel':"I am doing well. I am ready and excited for break!",
'Nate':"I'm just vibing right now, ready for break ",
'Jacob':"I am going to eat Turkey.",
'Anon':"nom nom nom"
}
# create a new count vecorizer so that we can return to the news analysis
count_vec_ex = text.CountVectorizer()
tfidf_ex = text.TfidfVectorizer()
mat_c = count_vec_ex.fit_transform(sentence_dict.values()).toarray()
mat_t = tfidf_ex.fit_transform(sentence_dict.values()).toarray()
# sentence_df = pd.DataFrame(data = mat_c, 
#                            columns =counts.get_feature_names_out(),
#                           index=sentence_dict.keys())

dist_df_count = pd.DataFrame(data = euclidean_distances(mat_c),
            index= sentence_dict.keys(), columns= sentence_dict.keys())
dist_df_tfidf = pd.DataFrame(data = euclidean_distances(mat_t),
            index= sentence_dict.keys(), columns= sentence_dict.keys())

ref = 'Professor Brown'
```

```{code-cell} ipython3
count_closest = dist_df_count[ref].drop(ref).idxmin()
count_closest
```

```{code-cell} ipython3
tfidf_closest = dist_df_tfidf[ref].drop(ref).idxmin()
tfidf_closest
```

```{code-cell} ipython3
sentence_dict[ref]
```

```{code-cell} ipython3
sentence_dict[count_closest]
```

```{code-cell} ipython3
sentence_dict[tfidf_closest]
```

Now, the closest sentence actually shares a wrod that's similar. 

```{code-cell} ipython3
dist_df_count[ref].drop(ref).sort_values()
```

```{code-cell} ipython3
dist_df_tfidf[ref].drop(ref).sort_values()
```

Now, the distances are all much smaller and Chris who also talked about Thanksgiving is much higher on the list too.  

```{code-cell} ipython3
mat_c.sum(axis=1)
```

```{code-cell} ipython3
mat_t.sum(axis=1)
```

The sums are again much closer so the total length isn't as much of the signal. Since it's smaller, we can also sum along words. 

```{code-cell} ipython3
sent_word_df = pd.DataFrame(data = count_vec_ex.get_feature_names(),
                           columns = ['word'])
sent_word_df['count_tot'] = mat_c.sum(axis=0)
sent_word_df['tfidf_tot'] = mat_t.sum(axis=0)
sent_word_df['doc_count'] = (mat_c>0).sum(axis=0)
```

```{code-cell} ipython3
sent_word_df.sort_values(by='doc_count',ascending=False)
```


## Sparse Matrices

To understand the sparse matrices, we can examine the size of the objects as a sparse matrix and after exporting to array.  

```{code-cell} ipython3
getsizeof(ng_vec_train[0])
```

```{code-cell} ipython3
getsizeof(ng_vec_train[0].toarray())
```

This is another reason it is better to use the sparse matrices (as returned by `transform`, but not by `fit_transform`)

```{code-cell} ipython3
ng_tfidf_train[0].toarray()
```

## More Practice

1. On the sentence dataset, try implementing the tf-idf calculation. 

```{code-cell} ipython3

```
