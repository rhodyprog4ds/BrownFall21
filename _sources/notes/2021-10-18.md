---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Making Predictions in Generative Model

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
iris_df = sns.load_dataset('iris')
```

We'll  load the data again

```{code-cell} ipython3
iris_df.head(1)
```

Next we indicate the feature variables and target and split into test and train
sets.  Well use 80% of the data for training.

```{code-cell} ipython3
feature_vars = ['sepal_length', 'sepal_width','petal_length', 'petal_width',]
target_var = 'species'

X_train, X_test, y_train, y_test = train_test_split(iris_df[feature_vars],
                          iris_df[target_var], train_size=.8, random_state=0)
```

We can confirm the shape is as expected

```{code-cell} ipython3
X_train.shape
```

```{code-cell} ipython3
iris_df.shape
```

```{code-cell} ipython3
.8*150
```

Next we again initialize and fit the classifier

```{code-cell} ipython3
gnb = GaussianNB()
gnb.fit(X_train, y_train)
```

We can compute predictions

```{code-cell} ipython3
y_pred = gnb.predict(X_test)
y_pred
```

And score the results

```{code-cell} ipython3
gnb.score(X_test,y_test)
```

We saw last week that when we fit the Gaussian Naive Bayes, it computes a mean
$\theta$ and variance $\sigma$ and adds them to model parameters in attributes
`gnb.theta_, gnb.sigma_`.

When we use the predict method, it uses those parameters to calculate the
likelihood of the sample according to a Gaussian distribution (normal) for each
class and then calculates the probability of the sample belonging to each class.

```{code-cell} ipython3
gnb.predict_proba(X_test)
```

These are hard to interpret as is, one option is to plot them

```{code-cell} ipython3
# make the prbabilities into a dataframe labeled with classes & make the index a separate column
prob_df = pd.DataFrame(data = gnb.predict_proba(X_test), columns = gnb.classes_ ).reset_index()
# add the predictions
prob_df['predicted_species'] = y_pred
prob_df['true_species'] = y_test.values
# for plotting, make a column that combines the index & prediction
pred_text = lambda r: str( r['index']) + ',' + r['predicted_species']
prob_df['i,pred'] = prob_df.apply(pred_text,axis=1)
# same for ground truth
true_text = lambda r: str( r['index']) + ',' + r['true_species']
prob_df['correct'] = prob_df['predicted_species'] == prob_df['true_species']
# a dd a column for which are correct
prob_df['i,true'] = prob_df.apply(true_text,axis=1)
prob_df_melted = prob_df.melt(id_vars =[ 'index', 'predicted_species','true_species','i,pred','i,true','correct'],value_vars = gnb.classes_,
                             var_name = target_var, value_name = 'probability')
prob_df_melted.head()
```

Now we have a data frame where each rown is one the probability of one sample belonging to one class. So there's a total of `number_of_samples*number_of_classes` rows

```{code-cell} ipython3
prob_df_melted.shape
```

```{code-cell} ipython3
len(y_pred)*len(gnb.classes_)
```

One way to look at these is to, for each sample in the test set, make a bar chart of the probability it belongs to each class.  We added to the data frame information so that we can plot this with the true class in the title using `col = 'i,true'`

````{margin}
```{tip}
I used `set_theme` to change both the fond size and the color palette.
Seaborn has a [detailed guide](https://seaborn.pydata.org/tutorial/color_palettes.html#palette-tutorial)
for choosing colors. The `colorblind` palette uses colors that are distinguishable under most common
forms of color blindness.
```
````

```{code-cell} ipython3
sns.set_theme(font_scale=2, palette= "colorblind")
# plot a bar graph for each point labeled with the prediction
sns.catplot(data =prob_df_melted, x = 'species', y='probability' ,col ='i,true',
            col_wrap=5,kind='bar')
```

We see that most sampples have nearly all of their probability mass (all probabiilties in a distribution sum (or integrate if continuous) to 1, but a few samples are not.  

```{admonition} Try it yourself
Try adding a column that could change the headings to include an indicator of which are correct or not)
```

For now, we'll group and look at on average, what the distributions are for correct vs incorrect based on predictions.

```{code-cell} ipython3
sns.set(font_scale=1.25, palette= "colorblind")
sns.catplot(data =prob_df_melted, x = 'species', y='probability' ,
            col ='predicted_species',row ='correct', kind='bar')
```

We see that the errors were all for versicolor, and on average the distribution is very uncertain for those samples.  Those samples are probably hard to distinguish. We could check by creating a data frame with the data and the information about predictions and correct values.

```{code-cell} ipython3
prob_data_df = pd.concat([prob_df,X_test.reset_index()],axis=1).drop(columns=['index'])
prob_data_df.head(2)
```

```{code-cell} ipython3
feature_vars
```

```{code-cell} ipython3

g = sns.PairGrid(prob_data_df,x_vars=feature_vars,y_vars= feature_vars,hue='true_species')

g.map_diag(sns.kdeplot)
g.map_offdiag(sns.scatterplot, size=prob_data_df["correct"])

g.add_legend()
```

Here we see that the large dots (the incorrect ones) are all nearby to points of a different color.  They were in fact samples that are similar to the other species.  So again, this result makes sense and helps us see when classifiers that are a good fit for the data will still make mistakes.

We can also look at the probabilites of the predicted sample using max

```{code-cell} ipython3
p_predicted = np.max(gnb.predict_proba(X_test),axis=1)
p_predicted
```

We see here that most of the predictions are pretty confident.
We can also use the probabilities to then compute predictions and compare these to what the `predict` method gave, to confirm that this is how the predict method works.

```{code-cell} ipython3
pd.DataFrame(data = gnb.predict_proba(X_test), columns = gnb.classes_ ).idxmax(axis=1) ==y_pred
```

```{code-cell} ipython3

```
