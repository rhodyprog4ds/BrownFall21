---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  name: python3
---

# Neural Networks with Keras

```{code-cell} ipython3

import numpy as np          # advanced math library
import matplotlib.pyplot as plt   
import random            # for generating random numbers

from keras.datasets import mnist  # MNIST dataset is included in Keras  
from keras.models import Sequential # Model type to be used

from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils             # NumPy related tools

from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from keras.layers import BatchNormalization
```

First, we'll use `keras` to build similar networks to the ones we saw with `sklearn`
it will be more complex, and we're using a different version of the digits data
(larger images) but this will still be just learning the predictions, not the
representation for now.  On Friday, we'll learn how to add new layers that
transform the data and learn the representation at the same time.

## Preparing data for deep learning

The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images. It's realated to the digits data that we have seen before.

We will load the data and look at a random sample.

```{code-cell} ipython3
(X_train, y_train), (X_test, y_test) = mnist.load_data()


plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger

for i in range(9):
  plt.subplot(3,3,i+1)
  num = random.randint(0, len(X_train))
  plt.imshow(X_train[num], cmap='gray', interpolation='none')
  plt.title("Class {}".format(y_train[num]))

plt.tight_layout()
```

Next, we need to transform the data to be compatible by reshaping 28 x 28 matrices into vectors, then converting to floats and normalizing.

```{code-cell}

28*28
```
Now that we know the length, we can transform.

```{code-cell} ipython3

X_train = X_train.reshape(60000, 784) # 60,000 training samples
X_test = X_test.reshape(10000, 784)  #  10,000  test samples

X_train = X_train.astype('float32')  # change integers to 32-bit floating point numbers
X_test = X_test.astype('float32')

X_train /= 255            # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
```

We will use one-hot encoding for the target variable, since our neural net's output layer is acutaly the probability distribution over the 10 digits, not a value from 0 to 9.

```{code-cell} ipython3

nb_classes = 10 # number of unique digits

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
```

```{code-cell} ipython3


y_train[0]
```

```{code-cell} ipython3

Y_train[0]
```

## Building a neural network in Keras

We will start with a network similar to what we have seen in `sklearn`. It will have a number of layers and, when predicting pass data from one layer to the next sequentially.  

```{code-cell} ipython3


model = Sequential()
```

Next wee add layers.  In `keras` what we saw as one neuron (connections + actiation) before is treated as two separate layers, where the size of the layer is the number of neurons.

```{code-cell}


model.add(Dense(512, input_shape=(784,)))
model.add(Activation('relu'))
```



```{code-cell}


model.add(Dense(512))
model.add(Activation('relu'))
```

```{code-cell}


model.add(Dense(10))
model.add(Activation('softmax'))
```

```{code-cell}

model.summary()
```

Keras also has parameters about the  optimization, like we saw before, but we set those with the `compile` method.  
```{code-cell}


model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

```{code-cell}

model.fit(X_train, Y_train,
     batch_size=128, epochs=5,
     verbose=1)
```

```{code-cell}

score = model.evaluate(X_test, Y_test)
score
```

## What kind of mistakes did it make?

```{code-cell}

predicted_prob = model.predict(X_test)
predicted_classes = np.argmax(predicted_prob,axis=1)


# Check which items we got right / wrong
correct_indices = np.nonzero(predicted_classes == y_test)[0]

incorrect_indices = np.nonzero(predicted_classes != y_test)[0]


plt.figure()
for i, correct in enumerate(correct_indices[:9]):
  plt.subplot(3,3,i+1)
  plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')
  plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_test[correct]))

plt.tight_layout()

plt.figure()
for i, incorrect in enumerate(incorrect_indices[:9]):
  plt.subplot(3,3,i+1)
  plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
  plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], y_test[incorrect]))

plt.tight_layout()
```


## Dropout for less overfitting

Dropout makes some of the neurons zero.

```{code-cell}


model_dropout = Sequential()

model_dropout.add(Dense(512, input_shape=(784,)))
model_dropout.add(Activation('relu'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(512))
model_dropout.add(Activation('relu'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(10))
model_dropout.add(Activation('softmax'))


model_dropout.summary()
```

Again, we set the optimization paramaters and then we can fit.

```{code-cell}
model_dropout.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dropout.fit(X_train, Y_train,
     batch_size=128, epochs=5,
     verbose=1)
```

```{code-cell}

score = model_dropout.evaluate(X_test, Y_test)
score
```

Now we see the gap between the train and test performance is smaller and the
test performance is higher.
