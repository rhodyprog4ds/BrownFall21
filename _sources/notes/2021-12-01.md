---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Neural Networks

We started thinking about machine learning wiht the idea that the basic idea is
that we assume that our target variable ($y_i$) is related to the features $\mathbf{x}_i$
by some function (for sample $i$):

$$ y_i =f(\mathbf{x}_i)$$

But we don't know that function exactly, so we assume a type of $f$ (a decision
  tree, a boundary for SVM, a propbility distribuiton) that has some paramters
  $\theta$ and then use a machine
  learning algorithm $\mathcal{A}$ to estimate the parameters for $f$.  In the
  decision tree the parameters are the thresholds to compare to, in the GaussianNB the parameters are the mean and variance, in SVM it's the support vecotrs that define the margin.  

$$\theta = \mathcal{A}(X,y) $$

That we can use to test on our test data:

$$ \hat{y}_i = f(x_i;\theta)

A neural net allows us to not assume a specific form for $f$ first, it does
universal funciton approximation.  For one hidden layer and a binary classification problem:


$$f(x) = W_2g(W_1^T x +b_1) + b_2 $$

where the function $g$ is called the activation function. so we approximate some
unkown, complicated function $f4 by taking a weighted sum of all of the inputs,
and passing those through another, known function.

```{code-cell} ipython3

from sklearn.neural_network import MLPClassifier
from sklearn import svm
import pandas as pd
import sklearn

from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn import model_selection
import numpy as np
```

We're going to use the digits dataset again.

```{code-cell} ipython3
digits = datasets.load_digits()
digits_X = digits.data
digits_y = digits.target
X_train, X_test, y_train, y_test = model_selection.train_test_split(digits_X,digits_y)
```


```{code-cell} ipython3
digits.images[0]
```

Sklearn provides an estimator for the MLP. We can see one with one layer to
start.

```{code-cell} ipython3
mlp = MLPClassifier(
  hidden_layer_sizes=(16),
  max_iter=500,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  random_state=1,
  learning_rate_init=0.1,
)
```

```{code-cell} ipython3
mlp.fit(X_train,y_train)
mlp.score(X_test,y_test)
```

We can also see what happens if we increase the size of the hidden layer.

```{code-cell} ipython3
mlp = MLPClassifier(
  hidden_layer_sizes=(64),
  max_iter=500,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  random_state=1,
  learning_rate_init=0.1,
)
```

```{code-cell} ipython3
mlp.fit(X_train,y_train)
mlp.score(X_test,y_test)
```

We can compare it  to SVM:

```{code-cell} ipython3
svm_clf = svm.SVC(gamma=0.001)
svm_clf.fit(X_train, y_train)
svm_clf.score(X_test,y_test)
```


We can also have multiple hidden layers:

```{code-cell} ipython3
mlp = MLPClassifier(
  hidden_layer_sizes=(64,64),
  max_iter=500,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  random_state=1,
  learning_rate_init=0.1,
)
mlp.fit(X_train,y_train)
mlp.score(X_test,y_test)
```

 We saw that the SVM performed a bit better, but this is a simple problem.
 We can also compare these based on much they store, the number of parameters
 is realted to the complexity.

```{code-cell} ipython3
svm_clf.support_vectors_.shape
```

```{code-cell} ipython3
[c.shape for c in mlp.coefs_]
```

```{code-cell} ipython3
np.prod(list(svm_clf.support_vectors_.shape))
```

```{code-cell} ipython3
np.sum([np.prod(list(c.shape)) for c in mlp.coefs_])
```

We see this is much smaler. 
