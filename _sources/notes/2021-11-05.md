---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Evaluating Clustering

```{code-cell} ipython3
import seaborn as sns
from sklearn import datasets
from sklearn.cluster import KMeans
from sklearn import metrics
import pandas as pd
```

We'll continue with the iris dataset because it's visually clear.
```{code-cell} ipython3
measurement_cols = ['sepal_length','petal_length','sepal_width','petal_width']

iris_df = sns.load_dataset('iris')
iris_X = iris_df[measurement_cols]
```
## Clusting with KMeans



```{code-cell} ipython3
km3 = KMeans(n_clusters=3)
```

```{code-cell} ipython3
km3.fit(iris_X)
```

```{code-cell} ipython3
km3.labels_
```

## Silhouette Score

One way to intutitively think about a good clustering solution is that every point should be close to points in the same cluster and far from points in other clusters. By definition with Kmeans, they will always be clos*er* to points in the same cluster, but we also what that the clusters aren't just touching, but actually spaced apart, if the clustering actually captures meaningfully different groups.

$$ s = \frac{b-a}{max(a,b)}$$

If our points are each an $x_i$, the assignments we found are a $z_i$, and the
means we found are $mu_k$ with $k = 0, 1, \ldots, K$

a: The mean distance between a sample and all other points in the same class.
$$ a = \frac{1}{N} \sum_i (x_i - mu_{z_i}) $

b: The mean distance between a sample and all other points in the next nearest cluster.


This score computes a ratio of how close points are to points in the same cluster vs other clusters

---
Use the `labels_` attribute of the kmeans objects to compare the silhouette score for K =2 and K=3.

```{code-cell} ipython3
metrics.silhouette_score(iris_X,km3.labels_)
```

```{code-cell} ipython3
km2 = KMeans(n_clusters=2)
km4 = KMeans(n_clusters=4)
km2.fit(iris_X)
km4.fit(iris_X)
```

```{code-cell} ipython3
metrics.silhouette_score(iris_X,km2.labels_)
```

```{code-cell} ipython3
metrics.silhouette_score(iris_X,km4.labels_)
```

```{code-cell} ipython3
iris_df['clusters3'] = km3.labels_
```

```{code-cell} ipython3
sns.pairplot(data= iris_df,hue= 'clusters3',)
# read docs to figure out hwy it didnt' plot clusters3
```

```{code-cell} ipython3

```


Other types of clustering: [sklearn overivew](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods)

---
[classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)

## Mutual Information

When we know the truth, we can see if the learned clusters are related to the true groups, we can't compare them like accuracy but we can use a metric that is intuitively like a correlation for categorical variables, the mutual information.

The `adjusted_mutual_info_score` methos in the `metrics` module computes a version of mutual information that is normalized to have good properties. Apply that to the two different clustering solutions and to a solution for K=4.


---

```python
metrics.adjusted_mutual_info_score(iris_df['species'],km2.labels_)
```
---

```python
metrics.adjusted_mutual_info_score(iris_df['species'],km3.labels_)
```

---

```python
metrics.adjusted_mutual_info_score(iris_df['species'],km4.labels_)
```

How does K=4 vs K=2 make different errors? how does mutual information help see that?


## Relationship between Tasks

We learned classification first, because it shares similarities with each
regression and clustering, while they share less.

Take a moment to think and talk with people around you:

- What do classification and regression share? How to they differ?
- What do classification and clustering share? How to they differ?
- How do these differences and similarities relate to what data we can use for
each and what types of questions we can ask?

## Questions After Class


How I would use all of those different ones.
How exactly do I find numbers of clusters


no questions
what do you mean by the data is represented in the 4th dimension
How do we utilize these clusters once we have them?
I still don't understand  how the algorithm works but I think it's too complex for me to understand....
how do we go from the labels giving elements of clusters to matching the appropriate feature variables
