---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---


# Intro to Machine learning

When we *do* machine learning, this can also be called:
- data mining
- pattern recognition
- modeling

because we are looking for patterns in the data and typically then planning to
use those patterns to make predictions or automate a task.  

Each of these terms does have slightly different meanings and usage, but
someitmes they're used close to exchangeably.


We're going to appraoch machine learning from the perspective of *modeling* for
a few reasons:
- model based machine learnining streamlines understanding the big picture
- the model way of interpreting it aligns well with using sklearn
- thinking in terms of models aligns with incorporating domain expertise, as in our data science definition


```{admonition} Further Reading
this [paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-MBML-2012.pdf) by Christopher M. Bishop, a leading ML researcher who also wrote one of a the widely preferred
graduate level ML textbooks, details advantages of a model based perspective and
a more mathematical version of a model based approach to machine learning.
```

````{margin}
```{hint}
In CSC461: Machine Learning, you can encounter an *algorithm* focused approach
to machine learning, but I think having the model based perspective first helps
you avoid common pitfalls.
```
````

## What is a Model?

A model is a simplified representation of some part of the world. A famous quote about models is:


```{epigraph}
All models are wrong, but some are useful
--[George Box](https://en.wikipedia.org/wiki/All_models_are_wrong)[^wiki]
```

In machine learning, we use models, that are generally _statistical_ models.


> A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process
[wikipedia](https://en.wikipedia.org/wiki/Statistical_model#:~:text=A%20statistical%20model%20is%20a,%2C%20the%20data%2Dgenerating%20process.)

```{admonition} Further Reading
read more in the[Model Based Machine Learning Book](http://www.mbmlbook.com/LearningSkills_A_model_is_a_set_of_assumptions.html)

```

## Models in Machine Learning

Starting from a dataset, we first make an additional designation about how we will use the different variables (columns). We will call most of them the _features_, which we denote mathematically with $\mathbf{X}$ and we'll choose one to be the _target_ or _labels_, denoted by $\mathbf{y}$.

The core assumption for just about all machine learning is that there exists some function $f$ so that for the $i$th sample

$$
  y_i = f(\mathbf{x}_i)
$$

## Types of Machine Learning

Then with different additional assumptions we get different types of machine learning:
- if both features ($\mathbf{X}$) and target ($\mathbf{y}$) are observed (contained in our dataset) it's [__supervised learning__](https://en.wikipedia.org/wiki/Supervised_learning) [code](https://scikit-learn.org/stable/supervised_learning.html)
- if only the features ($\mathbf{X}$) are observed, it's [__unsupervised learning__](https://en.wikipedia.org/wiki/Unsupervised_learning) [code](https://scikit-learn.org/stable/unsupervised_learning.html)


## Supervised Learning

we'll focus on supervised learning first.  we can take that same core assumption and use it with additional information about our target variable to determine learning __task__ we are working to do.

$$
  y_i = f(\mathbf{x}_i)
$$

- if $y_i$ are discrete (eg flower species) we are doing __classification__
- if $y_i$ are continuous (eg height) we are doing __regression__


## Machine Learning Pipeline

To do machine learning we start with __training data__ which we put as input to the __learning algorithm__. A learning algorithm might be a generic optimization procedure or a specialized procedure for a specific model. The learning algorithm outputs a trained __model__ or the parameters of the model. When we deploy a model we pair the __fit model__ with a __prediction algorithm__ or __decision__ algorithm to evaluate a new sample in the world.

In experimenting and design, we need __testing data__ to evaluate how well our learning algorithm understood the world.  We need to use previously unseen data, because if we don't we can't tell if the prediction algorithm is using a rule that the learning algorithm produced or just looking up from a lookup table the result.  This can be thought of like the difference between memorization and understanding.

When the model does well on the training data, but not on test data, we say that it does not generalize well.  


## Machine Learning with Scikit Learn

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
```

```{code-cell} ipython3
type(GaussianNB)
```

```{code-cell} ipython3
type(train_test_split)
```
We're trying to build an automatic flower classifier that, for measurements of a new flower returns the predicted species. To do this, we have a DataFrame with columns for species, petal width, petal length, sepal length, and sepal width. The species is what type of flower it is the petal and sepal are parts of the flower.

We'll load the data from Seaborn
```{code-cell} ipython3
iris_df = sns.load_dataset('iris')
```

First, we'll look at the data.
```{code-cell} ipython3
sns.pairplot(data=iris_df, hue='species')
```

We can see that this data is well suited for classification not only because
in the world it makes sense, that we can use the measurements of flower petals
to differentiate species, but also because the different species do not overlap
too much.  We call this **separable** data.

We'll pick out the target and features from this data frame
```{code-cell} ipython3
target = iris_df['species'].values
type(target)
```

```{code-cell} ipython3
features = iris_df.values[:,:4]
features.shape
```

Next, we use the sklearn function to create train and test data.
```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(features, target,random_state=0)
```

```{admonition} Try it Yourself
Try using a different random seed (value for `random_state`) or a different split
size.  Also try splitting by taking the first 80% of the data vs the last 20%.
What happens?
```

We'll check what the split does by looking at the shape.
```{code-cell} ipython3
X_train.shape
```

```{code-cell} ipython3
X_test.shape
```

Now we instantiate our classifier with the constructor method
````{margin}
```{admonition} Try it Yourself
1. what other constructors have we used?
1. What happens if you skip this step?
1. Can you cascade this step?
````

```{code-cell} ipython3
gnb = GaussianNB()
```

```{code-cell} ipython3
gnb
```
This created an empty object, we can look at it's contents:
```{code-cell} ipython3
gnb.__dict__
```

We can fit our model, or learn the model parameters, with the `fit` method.  THe
fit method implements the actual learning algorithm.
```{code-cell} ipython3
gnb.fit(X_train, y_train)
```

Once we fit we can see it knows about our dataset now
```{code-cell} ipython3
gnb.__dict__
```

We can apply our classifier with the predict method, which generates a prediction
based on what it learned from the training data for each sample in the test data.
```{code-cell} ipython3
y_pred = gnb.predict(X_test)
```

```{code-cell} ipython3
y_pred
```
We'll see better ways to evaluate on Friday, but as a first check, we can check
if it matches
```{code-cell} ipython3
sum(y_pred == y_test)
```
and compare to how many.
```{code-cell} ipython3
len(y_test)
```
