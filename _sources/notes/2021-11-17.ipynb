{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a27eb853",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "We'll pick up from where we left off on Monday.\n",
    "\n",
    "````{margin}\n",
    "```{admonition} Further Reading\n",
    "\n",
    "If you struggled to understand this code excerpt to fill in the comments, some generic strategies to understand code may help, beyond applying what we have covered in class.\n",
    "\n",
    "The Programmer's brain\n",
    "is an overview of how brains work, as applied to programming, written for working\n",
    "developers. This means that it assumes you know most CS concepts and at least two programming languages. If you don't there may be some parts that do not make sense to you, but the general ideas should still make sense.\n",
    "The author is a professor who researchers how people learn\n",
    "programming and how to effectively teach it.\n",
    "\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5522b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import cluster\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "# import the whole model selection module\n",
    "from sklearn import model_selection\n",
    "sns.set_theme(palette='colorblind')\n",
    "\n",
    "# load and split the data\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = model_selection.train_test_split(\n",
    "  iris_X,iris_y, test_size =.2)\n",
    "\n",
    "\n",
    "# create dt, set param grid & create optimizer\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "params_dt = {'criterion':['gini','entropy'],\n",
    "       'max_depth':[2,3,4,5,6],\n",
    "    'min_samples_leaf':list(range(2,20,2))}\n",
    "\n",
    "dt_opt = model_selection.GridSearchCV(dt,params_dt,cv=10)\n",
    "\n",
    "\n",
    "# fit the model and optimize\n",
    "dt_opt.fit(iris_X_train,iris_y_train)\n",
    "\n",
    "# store the resutl sin a dataframe\n",
    "dt_df = pd.DataFrame(dt_opt.cv_results_)\n",
    "\n",
    "\n",
    "# create svm, its parameter grid and optimizer\n",
    "svm_clf = svm.SVC()\n",
    "param_grid = {'kernel':['linear','rbf'], 'C':[.5, .75,1,2,5,7, 10]}\n",
    "svm_opt = model_selection.GridSearchCV(svm_clf,param_grid,cv=10)\n",
    "\n",
    "# fit the model and put the CV results in a dataframe\n",
    "svm_opt.fit(iris_X_train,iris_y_train)\n",
    "sv_df = pd.DataFrame(svm_opt.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b771f",
   "metadata": {},
   "source": [
    "We can compare how the best models fit during validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea88302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9833333333333332, 0.9583333333333333)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_opt.best_score_, dt_opt.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43593f",
   "metadata": {},
   "source": [
    "We see that the SVM does a little bit better.\n",
    "\n",
    "We can also apply the best estimator from each model class (that is the best parameter settings for each SVM and Decision Tree) to the test data to get our final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3f5f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_opt.best_estimator_.score(iris_X_test,iris_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f6a037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_opt.best_estimator_.score(iris_X_test,iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d9b89",
   "metadata": {},
   "source": [
    "We can also examine the results to think through this choice more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df74181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00072</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'linear'}</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00083</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.5</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.5, 'kernel': 'rbf'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.040825</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.00072      0.000050         0.000322        0.000025     0.5   \n",
       "1        0.00083      0.000037         0.000370        0.000019     0.5   \n",
       "\n",
       "  param_kernel                          params  split0_test_score  \\\n",
       "0       linear  {'C': 0.5, 'kernel': 'linear'}           0.916667   \n",
       "1          rbf     {'C': 0.5, 'kernel': 'rbf'}           1.000000   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           1.000000           0.916667                1.0                1.0   \n",
       "1           0.916667           0.916667                1.0                1.0   \n",
       "\n",
       "   split5_test_score  split6_test_score  split7_test_score  split8_test_score  \\\n",
       "0                1.0           1.000000                1.0           1.000000   \n",
       "1                1.0           0.916667                1.0           0.916667   \n",
       "\n",
       "   split9_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0                1.0         0.983333        0.033333                1  \n",
       "1                1.0         0.966667        0.040825               10  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037b131",
   "metadata": {},
   "source": [
    "We can  use EDA to understand how the score varied across all of the parameter settings we tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae26aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14.000000\n",
       "mean      0.975000\n",
       "std       0.007309\n",
       "min       0.966667\n",
       "25%       0.966667\n",
       "50%       0.975000\n",
       "75%       0.983333\n",
       "max       0.983333\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_df['mean_test_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1eb3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    90.000000\n",
       "mean      0.957222\n",
       "std       0.003789\n",
       "min       0.941667\n",
       "25%       0.958333\n",
       "50%       0.958333\n",
       "75%       0.958333\n",
       "max       0.958333\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_df['mean_test_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809b73e",
   "metadata": {},
   "source": [
    "From this we see that in both cases the standard deviation (std) is really\n",
    "low. This tells us that the parameter changes didn't impact the performance\n",
    "much.  Combined with the overall high accuracy this tells us that the data\n",
    "is probably really easy to classify.  If the performance had been uniformly\n",
    "bad, it might have instead told us that we did not try a wide enough range\n",
    "of parameters.\n",
    "\n",
    "To confirm how many parameter settings we have used we can check a couple different ways. First, above in the count of the describe.\n",
    "\n",
    "We can also calculate directly from the parameter grids before we even do the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292d5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_combinations = 1\n",
    "for param, vals in param_grid.items():\n",
    "    n_combinations *= len(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fbe07a",
   "metadata": {},
   "source": [
    "## When do differences matter?\n",
    "\n",
    "We can check calculate a confidence interval to determine more precisely when the performance of two models is meaningfully different.  \n",
    "\n",
    "\n",
    "This function calculates the 95% confidence interval.  The range within which we are 95% confident the quantity we have estimated is truly within in.  When we have more samples in the test set used to calculate the score, we are more confident in the estimate, so the interval is narrower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153c7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_confint(acc, n):\n",
    "    '''\n",
    "    Compute the 95% confidence interval for a classification problem.\n",
    "    acc -- classification accuracy\n",
    "    n  -- number of observations used to compute the accuracy\n",
    "    Returns a tuple (lb,ub)\n",
    "    '''\n",
    "    interval = 1.96*np.sqrt(acc*(1-acc)/n)\n",
    "    lb = max(0, acc - interval)\n",
    "    ub = min(1.0, acc + interval)\n",
    "    return (lb,ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bda983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9833333333333332, 0.9583333333333333)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_opt.best_score_, dt_opt.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bce048",
   "metadata": {},
   "source": [
    "We can calculate the number of observations used to compute the accuracy using the size of the training data and the fact that we set it to 10-fold cross validation. That means that 10% (100/10) of the data was used for each fold and each validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a048624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iris_X_train)*.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "529caa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split0_score = dt_df['split0_test_score'][dt_opt.best_index_]\n",
    "classification_confint(split0_score ,len(iris_X_train)*.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5ec98",
   "metadata": {},
   "source": [
    "```{admonition} Correction\n",
    "since the best score is cross validated it actually uses the whole training data (by averaging 10 scores together).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f23f083a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9225798693726877, 0.9940867972939789)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_confint( dt_opt.best_score_,len(iris_X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c65a96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iris_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a9c947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6999722251151381, 0.9666944415515286)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_confint(dt_opt.best_estimator_.score(iris_X_test,iris_y_test),len(iris_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fa20a",
   "metadata": {},
   "source": [
    "If we take the exact value we can see how more samples narrows the interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19df0df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8266860375572443, 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_confint(.95,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e73d761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8720094022760863, 1.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_confint(.95,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2f6b8",
   "metadata": {},
   "source": [
    "We can further explore this by plotting directly from the calculation out of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee97d116",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2188489612.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_2452/2188489612.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    n_list = list(range(5,200)])\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_list = list(range(5,200)])\n",
    "intervals = [1.96*np.sqrt(acc*(1-acc)/n) for n in n_list]\n",
    "plt.plot(n_list, intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57829061",
   "metadata": {},
   "source": [
    "## Questions After Class\n",
    "\n",
    "### When would I choose to use an svm?\n",
    "```{toggle}\n",
    "SVMs are often very accurate, and while in some cases a decision tree may be considered more interpretable, an SVM is not necessarily too hard to interpret.\n",
    "\n",
    "If accuracy is important, the SVM is often a really good choice.\n",
    "```\n",
    "\n",
    "### How can we tell how many parameter settings we tried?\n",
    "```{toggle}\n",
    "\n",
    "We can calculate it in advance from the parameter grid or the length of the output DataFrame/lists in the dictionary.  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "source_map": [
   12,
   32,
   76,
   80,
   82,
   86,
   90,
   92,
   95,
   97,
   101,
   105,
   107,
   119,
   123,
   133,
   147,
   149,
   153,
   157,
   160,
   168,
   172,
   176,
   178,
   181,
   185,
   187,
   190,
   194
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}