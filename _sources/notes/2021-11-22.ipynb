{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa31215",
   "metadata": {},
   "source": [
    "# Intro to NLP- representing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbd4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea82ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load http://drsmb.co/310read\n",
    "\n",
    "# add an entry to the following dictionary with a name as the key and a sentence as the value\n",
    "# share a sentence about how you're doing this week\n",
    "# remember this will be python code, don't use\n",
    "# You can remain anonymous (this page & the notes will be fully public)\n",
    "# by attributing it to a celebrity or psuedonym, but include *some* sort of attribution\n",
    "sentence_dict = {\n",
    "'Professor Brown':\"I'm excited for Thanksgiving.\",\n",
    "'Matt Langton':\"I'm doing pretty good, I'll be taking the days off to catch up on various classwork.\",\n",
    "'Evan':\"I'm just here so my grade doesn't get fined\",\n",
    "'Greg Bassett':\"I'm doing well, my birthday is today. I'm looking forward to seeing my family this Thursday, I haven't seen a lot of them in a long time.\",\n",
    "'Noah N':\"I'm doing well! I can't wait to take opportuity of this long weekend to catch up on various HW's, projects, etc.\",\n",
    "'Tuyetlinh':\"I'm struggling to get all my work done before break, but I'm excited to have that time off when I'm all done.\",\n",
    "'Kenza Bouabdallah':\"I am doing good. How are you ?\",\n",
    "'Chris Kerfoot':\"I'm doing pretty good. I'm happy to have some days off this week because of Thanksgiving!\",\n",
    "'Kang Liu': \"New week, new start\",\n",
    "'Aiden Hill':\"I am very much enjoying this class.\",\n",
    "'Muhammad S':\"I am doing pretty well. I am looking forward to taking a few days off.\",\n",
    "'Max Mastrorocco':\"Cannot wait for a break.\",\n",
    "'Daniel':\"I am doing well. I am ready and excited for break!\",\n",
    "'Nate':\"I'm just vibing right now, ready for break \",\n",
    "'Jacob':\"I am going to eat Turkey.\",\n",
    "'Anon':\"nom nom nom\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d34793",
   "metadata": {},
   "source": [
    "How can we analyze these? All of the machine leanring models we have seen only use numerical features organized into a table with one row per samplea and one column per feature.\n",
    "\n",
    "That's actually generally true.  ALl ML models require numerical features, at some point. The process of taking data that is not numerical and tabular, which is called unstrucutred, into strucutred (tabular) format we require is called feature extraction.  There are many, many ways to do that.  We'll see a few over the course of the rest of the semester.  Some more advanced models hide the feature extraction, by putting it in the same function, but it's always there.\n",
    "\n",
    "## Terms\n",
    "\n",
    "\n",
    "- document: unit of text we’re analyzing (one sample)\n",
    "- token: sequence of characters in some particular document that are grouped together as a useful semantic unit for processing (basically a word)\n",
    "- stop words: no meaning, we don’t need them (like a, the, an,). Note that this is context dependent\n",
    "- dictionary: all of the possible words that a given system knows how to process\n",
    "\n",
    "We'll start by taking out one sentence and anlyzeing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fc8b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm excited for Thanksgiving.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = sentence_dict['Professor Brown']\n",
    "\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc0b76",
   "metadata": {},
   "source": [
    "## Bag of Words Representionat\n",
    "\n",
    "We're going to learn a represetnation called the bag of words.  It ignores the order of the words within a document. To do this, we'll first extract all of the tokens (tokenize) the docuemtns and then count how mnay times each word appears.  This will be our numerical representation of the data.  \n",
    "\n",
    "````{margin}\n",
    "```{admotion} Further Reading\n",
    "[Transformers](https://scikit-learn.org/stable/data_transforms.html) are another broad class of sklearn objects.  We've seen Estimators mostly so far.\n",
    "We're focusing on the [text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for now.\n",
    "```\n",
    "````\n",
    "Then we initialize our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416b2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db44c32",
   "metadata": {},
   "source": [
    "We can use the fit transform method to fit the vectorizer model and apply it to this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4c86f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.fit_transform([s1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28db4f1",
   "metadata": {},
   "source": [
    "To see the output better, we use the toarray method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b24d96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.fit_transform([s1]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ebbc90",
   "metadata": {},
   "source": [
    "We can also examine attributes of the object.\n",
    "````{margin}\n",
    "```{tip}\n",
    "Notice that we keep using the same tools over and over to explore how things work.  You can do this on your own, when you're learning new things. Example\n",
    "code is readily available online but not all of it is well documented or\n",
    "clearly explained.\n",
    "\n",
    "Also, in a job, much, much, more of your time will be spent reading code than writing code from scratch. These strategies will help you get familiar with a new code base and get up to speed faster.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab49f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'excited': 0, 'for': 1, 'thanksgiving': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870ff5e",
   "metadata": {},
   "source": [
    "We see that what it does is creates an ordered (the values are the order) list of words as the parameters of this model (ending in `_` is an attribute of the object or parameter of the model)\n",
    "\n",
    "```{admonition} Try it yourself\n",
    "What other model parameters have we seen? How have we used model parameters in the past?\n",
    "```\n",
    "\n",
    "To see what happens a bit more, let's add a second senntence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "113bc0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New week, new start'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = sentence_dict['Kang Liu']\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab67bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'excited': 0, 'for': 1, 'thanksgiving': 4, 'new': 2, 'week': 5, 'start': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.fit_transform([s1,s2])\n",
    "counts.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb3ca1",
   "metadata": {},
   "source": [
    "Now we can see that it puts the words in the `vocabulary_` attribute (aka the {term}`dictionary`) in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b211b8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 2, 1, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.fit_transform([s1,s2]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7447180",
   "metadata": {},
   "source": [
    "From this we can see that the representation is the count of how many times each word appears.\n",
    "\n",
    "Now we can apply it to all of the sentences, or our whole {term}`corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8453571e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = counts.fit_transform(sentence_dict.values()).toarray()\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3c68e",
   "metadata": {},
   "source": [
    "We can get the dictionary out in order using the `get_feature_names` method. This method has a generic name, not specific to text, because it's a property of transformers in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e2d64c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'am',\n",
       " 'and',\n",
       " 'are',\n",
       " 'be',\n",
       " 'because',\n",
       " 'before',\n",
       " 'birthday',\n",
       " 'break',\n",
       " 'but',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'catch',\n",
       " 'class',\n",
       " 'classwork',\n",
       " 'days',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'eat',\n",
       " 'enjoying',\n",
       " 'etc',\n",
       " 'excited',\n",
       " 'family',\n",
       " 'few',\n",
       " 'fined',\n",
       " 'for',\n",
       " 'forward',\n",
       " 'get',\n",
       " 'going',\n",
       " 'good',\n",
       " 'grade',\n",
       " 'happy',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'here',\n",
       " 'how',\n",
       " 'hw',\n",
       " 'in',\n",
       " 'is',\n",
       " 'just',\n",
       " 'll',\n",
       " 'long',\n",
       " 'looking',\n",
       " 'lot',\n",
       " 'much',\n",
       " 'my',\n",
       " 'new',\n",
       " 'nom',\n",
       " 'now',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'opportuity',\n",
       " 'pretty',\n",
       " 'projects',\n",
       " 'ready',\n",
       " 'right',\n",
       " 'seeing',\n",
       " 'seen',\n",
       " 'so',\n",
       " 'some',\n",
       " 'start',\n",
       " 'struggling',\n",
       " 'take',\n",
       " 'taking',\n",
       " 'thanksgiving',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'this',\n",
       " 'thursday',\n",
       " 'time',\n",
       " 'to',\n",
       " 'today',\n",
       " 'turkey',\n",
       " 'up',\n",
       " 'various',\n",
       " 'very',\n",
       " 'vibing',\n",
       " 'wait',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'well',\n",
       " 'when',\n",
       " 'work',\n",
       " 'you']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d6608",
   "metadata": {},
   "source": [
    "We can use a dataframe again to see this more easily. We can put labels on both the index and the column headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375aa272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>be</th>\n",
       "      <th>because</th>\n",
       "      <th>before</th>\n",
       "      <th>birthday</th>\n",
       "      <th>break</th>\n",
       "      <th>but</th>\n",
       "      <th>...</th>\n",
       "      <th>various</th>\n",
       "      <th>very</th>\n",
       "      <th>vibing</th>\n",
       "      <th>wait</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>well</th>\n",
       "      <th>when</th>\n",
       "      <th>work</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Professor Brown</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matt Langton</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greg Bassett</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noah N</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuyetlinh</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenza Bouabdallah</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chris Kerfoot</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kang Liu</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aiden Hill</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Muhammad S</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max Mastrorocco</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacob</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   all  am  and  are  be  because  before  birthday  break  \\\n",
       "Professor Brown      0   0    0    0   0        0       0         0      0   \n",
       "Matt Langton         0   0    0    0   1        0       0         0      0   \n",
       "Evan                 0   0    0    0   0        0       0         0      0   \n",
       "Greg Bassett         0   0    0    0   0        0       0         1      0   \n",
       "Noah N               0   0    0    0   0        0       0         0      0   \n",
       "Tuyetlinh            2   0    0    0   0        0       1         0      1   \n",
       "Kenza Bouabdallah    0   1    0    1   0        0       0         0      0   \n",
       "Chris Kerfoot        0   0    0    0   0        1       0         0      0   \n",
       "Kang Liu             0   0    0    0   0        0       0         0      0   \n",
       "Aiden Hill           0   1    0    0   0        0       0         0      0   \n",
       "Muhammad S           0   2    0    0   0        0       0         0      0   \n",
       "Max Mastrorocco      0   0    0    0   0        0       0         0      1   \n",
       "Daniel               0   2    1    0   0        0       0         0      1   \n",
       "Nate                 0   0    0    0   0        0       0         0      1   \n",
       "Jacob                0   1    0    0   0        0       0         0      0   \n",
       "Anon                 0   0    0    0   0        0       0         0      0   \n",
       "\n",
       "                   but  ...  various  very  vibing  wait  week  weekend  well  \\\n",
       "Professor Brown      0  ...        0     0       0     0     0        0     0   \n",
       "Matt Langton         0  ...        1     0       0     0     0        0     0   \n",
       "Evan                 0  ...        0     0       0     0     0        0     0   \n",
       "Greg Bassett         0  ...        0     0       0     0     0        0     1   \n",
       "Noah N               0  ...        1     0       0     1     0        1     1   \n",
       "Tuyetlinh            1  ...        0     0       0     0     0        0     0   \n",
       "Kenza Bouabdallah    0  ...        0     0       0     0     0        0     0   \n",
       "Chris Kerfoot        0  ...        0     0       0     0     1        0     0   \n",
       "Kang Liu             0  ...        0     0       0     0     1        0     0   \n",
       "Aiden Hill           0  ...        0     1       0     0     0        0     0   \n",
       "Muhammad S           0  ...        0     0       0     0     0        0     1   \n",
       "Max Mastrorocco      0  ...        0     0       0     1     0        0     0   \n",
       "Daniel               0  ...        0     0       0     0     0        0     1   \n",
       "Nate                 0  ...        0     0       1     0     0        0     0   \n",
       "Jacob                0  ...        0     0       0     0     0        0     0   \n",
       "Anon                 0  ...        0     0       0     0     0        0     0   \n",
       "\n",
       "                   when  work  you  \n",
       "Professor Brown       0     0    0  \n",
       "Matt Langton          0     0    0  \n",
       "Evan                  0     0    0  \n",
       "Greg Bassett          0     0    0  \n",
       "Noah N                0     0    0  \n",
       "Tuyetlinh             1     1    0  \n",
       "Kenza Bouabdallah     0     0    1  \n",
       "Chris Kerfoot         0     0    0  \n",
       "Kang Liu              0     0    0  \n",
       "Aiden Hill            0     0    0  \n",
       "Muhammad S            0     0    0  \n",
       "Max Mastrorocco       0     0    0  \n",
       "Daniel                0     0    0  \n",
       "Nate                  0     0    0  \n",
       "Jacob                 0     0    0  \n",
       "Anon                  0     0    0  \n",
       "\n",
       "[16 rows x 87 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df = pd.DataFrame(data = mat, columns =counts.get_feature_names(),\n",
    "                          index=sentence_dict.keys())\n",
    "\n",
    "sentence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b19eb",
   "metadata": {},
   "source": [
    "## How can we find the most commonly used word?\n",
    "\n",
    "One guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51ae63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all        2\n",
       "am         2\n",
       "and        1\n",
       "are        1\n",
       "be         1\n",
       "          ..\n",
       "weekend    1\n",
       "well       1\n",
       "when       1\n",
       "work       1\n",
       "you        1\n",
       "Length: 87, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f09eb",
   "metadata": {},
   "source": [
    "This is the maximum number of times each word appears in single \"document\", but it's also not sorted, it's alphabetical.\n",
    "\n",
    "This shows the word that appears the most times.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "308a8274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "looking    1\n",
       "start      1\n",
       "some       1\n",
       "so         1\n",
       "seen       1\n",
       "          ..\n",
       "my         2\n",
       "done       2\n",
       "am         2\n",
       "all        2\n",
       "nom        3\n",
       "Length: 87, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.max().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503d92c",
   "metadata": {},
   "source": [
    "To get what we want we need to sum, which by default is along the columns, or per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c6dd6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all        2\n",
       "am         7\n",
       "and        1\n",
       "are        1\n",
       "be         1\n",
       "          ..\n",
       "weekend    1\n",
       "well       4\n",
       "when       1\n",
       "work       1\n",
       "you        1\n",
       "Length: 87, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd3aa8",
   "metadata": {},
   "source": [
    "Agaain it's unsorted, but we can apply max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0698da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame._add_numeric_operations.<locals>.max of all        2\n",
       "am         7\n",
       "and        1\n",
       "are        1\n",
       "be         1\n",
       "          ..\n",
       "weekend    1\n",
       "well       4\n",
       "when       1\n",
       "work       1\n",
       "you        1\n",
       "Length: 87, dtype: int64>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.sum().max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8a22a",
   "metadata": {},
   "source": [
    "this gives only t the value though, we want the word. When we summed we got back a Series with the words in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5336ef03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all', 'am', 'and', 'are', 'be', 'because', 'before', 'birthday',\n",
       "       'break', 'but', 'can', 'cannot', 'catch', 'class', 'classwork', 'days',\n",
       "       'doesn', 'doing', 'done', 'eat', 'enjoying', 'etc', 'excited', 'family',\n",
       "       'few', 'fined', 'for', 'forward', 'get', 'going', 'good', 'grade',\n",
       "       'happy', 'have', 'haven', 'here', 'how', 'hw', 'in', 'is', 'just', 'll',\n",
       "       'long', 'looking', 'lot', 'much', 'my', 'new', 'nom', 'now', 'of',\n",
       "       'off', 'on', 'opportuity', 'pretty', 'projects', 'ready', 'right',\n",
       "       'seeing', 'seen', 'so', 'some', 'start', 'struggling', 'take', 'taking',\n",
       "       'thanksgiving', 'that', 'the', 'them', 'this', 'thursday', 'time', 'to',\n",
       "       'today', 'turkey', 'up', 'various', 'very', 'vibing', 'wait', 'week',\n",
       "       'weekend', 'well', 'when', 'work', 'you'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.sum().index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c6453",
   "metadata": {},
   "source": [
    "So we can use idxmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7979cbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.sum().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b3928",
   "metadata": {},
   "source": [
    "## Distances in text\n",
    "We can now use a distance function to calculate how far apaart the different sentences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57495d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.24264069, 3.31662479, 5.19615242, 4.89897949,\n",
       "        5.09901951, 3.        , 3.87298335, 3.        , 3.        ,\n",
       "        4.12310563, 2.23606798, 3.16227766, 2.82842712, 2.82842712,\n",
       "        3.46410162],\n",
       "       [4.24264069, 0.        , 4.79583152, 5.91607978, 4.69041576,\n",
       "        5.83095189, 4.12310563, 4.12310563, 4.58257569, 4.58257569,\n",
       "        4.12310563, 4.35889894, 4.89897949, 4.69041576, 4.24264069,\n",
       "        4.89897949],\n",
       "       [3.31662479, 4.79583152, 0.        , 5.29150262, 5.38516481,\n",
       "        5.38516481, 3.74165739, 4.69041576, 3.74165739, 3.74165739,\n",
       "        4.69041576, 3.46410162, 4.35889894, 3.60555128, 3.60555128,\n",
       "        4.12310563],\n",
       "       [5.19615242, 5.91607978, 5.29150262, 0.        , 5.56776436,\n",
       "        6.244998  , 5.29150262, 5.47722558, 5.47722558, 5.29150262,\n",
       "        5.29150262, 5.29150262, 5.56776436, 5.56776436, 5.19615242,\n",
       "        5.74456265],\n",
       "       [4.89897949, 4.69041576, 5.38516481, 5.56776436, 0.        ,\n",
       "        6.164414  , 5.        , 5.        , 5.19615242, 5.        ,\n",
       "        5.19615242, 4.79583152, 5.29150262, 5.29150262, 4.69041576,\n",
       "        5.47722558],\n",
       "       [5.09901951, 5.83095189, 5.38516481, 6.244998  , 6.164414  ,\n",
       "        0.        , 5.56776436, 5.56776436, 5.56776436, 5.56776436,\n",
       "        5.74456265, 5.19615242, 5.65685425, 5.47722558, 5.09901951,\n",
       "        5.83095189],\n",
       "       [3.        , 4.12310563, 3.74165739, 5.29150262, 5.        ,\n",
       "        5.56776436, 0.        , 4.        , 3.46410162, 3.16227766,\n",
       "        3.74165739, 3.16227766, 3.31662479, 3.60555128, 3.        ,\n",
       "        3.87298335],\n",
       "       [3.87298335, 4.12310563, 4.69041576, 5.47722558, 5.        ,\n",
       "        5.56776436, 4.        , 0.        , 4.24264069, 4.24264069,\n",
       "        4.24264069, 4.24264069, 4.79583152, 4.58257569, 4.12310563,\n",
       "        4.79583152],\n",
       "       [3.        , 4.58257569, 3.74165739, 5.47722558, 5.19615242,\n",
       "        5.56776436, 3.46410162, 4.24264069, 0.        , 3.46410162,\n",
       "        4.47213595, 3.16227766, 4.12310563, 3.60555128, 3.31662479,\n",
       "        3.87298335],\n",
       "       [3.        , 4.58257569, 3.74165739, 5.29150262, 5.        ,\n",
       "        5.56776436, 3.16227766, 4.24264069, 3.46410162, 0.        ,\n",
       "        4.        , 3.16227766, 3.60555128, 3.60555128, 3.        ,\n",
       "        3.87298335],\n",
       "       [4.12310563, 4.12310563, 4.69041576, 5.29150262, 5.19615242,\n",
       "        5.74456265, 3.74165739, 4.24264069, 4.47213595, 4.        ,\n",
       "        0.        , 4.24264069, 3.60555128, 4.58257569, 3.60555128,\n",
       "        4.79583152],\n",
       "       [2.23606798, 4.35889894, 3.46410162, 5.29150262, 4.79583152,\n",
       "        5.19615242, 3.16227766, 4.24264069, 3.16227766, 3.16227766,\n",
       "        4.24264069, 0.        , 3.31662479, 2.64575131, 3.        ,\n",
       "        3.60555128],\n",
       "       [3.16227766, 4.89897949, 4.35889894, 5.56776436, 5.29150262,\n",
       "        5.65685425, 3.31662479, 4.79583152, 4.12310563, 3.60555128,\n",
       "        3.60555128, 3.31662479, 0.        , 3.46410162, 3.46410162,\n",
       "        4.47213595],\n",
       "       [2.82842712, 4.69041576, 3.60555128, 5.56776436, 5.29150262,\n",
       "        5.47722558, 3.60555128, 4.58257569, 3.60555128, 3.60555128,\n",
       "        4.58257569, 2.64575131, 3.46410162, 0.        , 3.46410162,\n",
       "        4.        ],\n",
       "       [2.82842712, 4.24264069, 3.60555128, 5.19615242, 4.69041576,\n",
       "        5.09901951, 3.        , 4.12310563, 3.31662479, 3.        ,\n",
       "        3.60555128, 3.        , 3.46410162, 3.46410162, 0.        ,\n",
       "        3.74165739],\n",
       "       [3.46410162, 4.89897949, 4.12310563, 5.74456265, 5.47722558,\n",
       "        5.83095189, 3.87298335, 4.79583152, 3.87298335, 3.87298335,\n",
       "        4.79583152, 3.60555128, 4.47213595, 4.        , 3.74165739,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(sentence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b0f7b",
   "metadata": {},
   "source": [
    "We can make this eaiser to read by making it a Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69aee08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Professor Brown</th>\n",
       "      <th>Matt Langton</th>\n",
       "      <th>Evan</th>\n",
       "      <th>Greg Bassett</th>\n",
       "      <th>Noah N</th>\n",
       "      <th>Tuyetlinh</th>\n",
       "      <th>Kenza Bouabdallah</th>\n",
       "      <th>Chris Kerfoot</th>\n",
       "      <th>Kang Liu</th>\n",
       "      <th>Aiden Hill</th>\n",
       "      <th>Muhammad S</th>\n",
       "      <th>Max Mastrorocco</th>\n",
       "      <th>Daniel</th>\n",
       "      <th>Nate</th>\n",
       "      <th>Jacob</th>\n",
       "      <th>Anon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Professor Brown</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>4.898979</td>\n",
       "      <td>5.099020</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>3.464102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matt Langton</th>\n",
       "      <td>4.242641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>5.916080</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>5.830952</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>4.898979</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.898979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evan</th>\n",
       "      <td>3.316625</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.385165</td>\n",
       "      <td>5.385165</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>4.123106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greg Bassett</th>\n",
       "      <td>5.196152</td>\n",
       "      <td>5.916080</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>6.244998</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>5.744563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noah N</th>\n",
       "      <td>4.898979</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>5.385165</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>5.477226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuyetlinh</th>\n",
       "      <td>5.099020</td>\n",
       "      <td>5.830952</td>\n",
       "      <td>5.385165</td>\n",
       "      <td>6.244998</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.744563</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>5.656854</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>5.099020</td>\n",
       "      <td>5.830952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenza Bouabdallah</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.872983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chris Kerfoot</th>\n",
       "      <td>3.872983</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.795832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kang Liu</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>4.472136</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.872983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aiden Hill</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.872983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Muhammad S</th>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>5.744563</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>4.472136</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>4.795832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max Mastrorocco</th>\n",
       "      <td>2.236068</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.605551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel</th>\n",
       "      <td>3.162278</td>\n",
       "      <td>4.898979</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.656854</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>4.472136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nate</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>5.567764</td>\n",
       "      <td>5.291503</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>4.582576</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacob</th>\n",
       "      <td>2.828427</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>5.099020</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.741657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anon</th>\n",
       "      <td>3.464102</td>\n",
       "      <td>4.898979</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>5.744563</td>\n",
       "      <td>5.477226</td>\n",
       "      <td>5.830952</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>4.472136</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Professor Brown  Matt Langton      Evan  Greg Bassett  \\\n",
       "Professor Brown           0.000000      4.242641  3.316625      5.196152   \n",
       "Matt Langton              4.242641      0.000000  4.795832      5.916080   \n",
       "Evan                      3.316625      4.795832  0.000000      5.291503   \n",
       "Greg Bassett              5.196152      5.916080  5.291503      0.000000   \n",
       "Noah N                    4.898979      4.690416  5.385165      5.567764   \n",
       "Tuyetlinh                 5.099020      5.830952  5.385165      6.244998   \n",
       "Kenza Bouabdallah         3.000000      4.123106  3.741657      5.291503   \n",
       "Chris Kerfoot             3.872983      4.123106  4.690416      5.477226   \n",
       "Kang Liu                  3.000000      4.582576  3.741657      5.477226   \n",
       "Aiden Hill                3.000000      4.582576  3.741657      5.291503   \n",
       "Muhammad S                4.123106      4.123106  4.690416      5.291503   \n",
       "Max Mastrorocco           2.236068      4.358899  3.464102      5.291503   \n",
       "Daniel                    3.162278      4.898979  4.358899      5.567764   \n",
       "Nate                      2.828427      4.690416  3.605551      5.567764   \n",
       "Jacob                     2.828427      4.242641  3.605551      5.196152   \n",
       "Anon                      3.464102      4.898979  4.123106      5.744563   \n",
       "\n",
       "                     Noah N  Tuyetlinh  Kenza Bouabdallah  Chris Kerfoot  \\\n",
       "Professor Brown    4.898979   5.099020           3.000000       3.872983   \n",
       "Matt Langton       4.690416   5.830952           4.123106       4.123106   \n",
       "Evan               5.385165   5.385165           3.741657       4.690416   \n",
       "Greg Bassett       5.567764   6.244998           5.291503       5.477226   \n",
       "Noah N             0.000000   6.164414           5.000000       5.000000   \n",
       "Tuyetlinh          6.164414   0.000000           5.567764       5.567764   \n",
       "Kenza Bouabdallah  5.000000   5.567764           0.000000       4.000000   \n",
       "Chris Kerfoot      5.000000   5.567764           4.000000       0.000000   \n",
       "Kang Liu           5.196152   5.567764           3.464102       4.242641   \n",
       "Aiden Hill         5.000000   5.567764           3.162278       4.242641   \n",
       "Muhammad S         5.196152   5.744563           3.741657       4.242641   \n",
       "Max Mastrorocco    4.795832   5.196152           3.162278       4.242641   \n",
       "Daniel             5.291503   5.656854           3.316625       4.795832   \n",
       "Nate               5.291503   5.477226           3.605551       4.582576   \n",
       "Jacob              4.690416   5.099020           3.000000       4.123106   \n",
       "Anon               5.477226   5.830952           3.872983       4.795832   \n",
       "\n",
       "                   Kang Liu  Aiden Hill  Muhammad S  Max Mastrorocco  \\\n",
       "Professor Brown    3.000000    3.000000    4.123106         2.236068   \n",
       "Matt Langton       4.582576    4.582576    4.123106         4.358899   \n",
       "Evan               3.741657    3.741657    4.690416         3.464102   \n",
       "Greg Bassett       5.477226    5.291503    5.291503         5.291503   \n",
       "Noah N             5.196152    5.000000    5.196152         4.795832   \n",
       "Tuyetlinh          5.567764    5.567764    5.744563         5.196152   \n",
       "Kenza Bouabdallah  3.464102    3.162278    3.741657         3.162278   \n",
       "Chris Kerfoot      4.242641    4.242641    4.242641         4.242641   \n",
       "Kang Liu           0.000000    3.464102    4.472136         3.162278   \n",
       "Aiden Hill         3.464102    0.000000    4.000000         3.162278   \n",
       "Muhammad S         4.472136    4.000000    0.000000         4.242641   \n",
       "Max Mastrorocco    3.162278    3.162278    4.242641         0.000000   \n",
       "Daniel             4.123106    3.605551    3.605551         3.316625   \n",
       "Nate               3.605551    3.605551    4.582576         2.645751   \n",
       "Jacob              3.316625    3.000000    3.605551         3.000000   \n",
       "Anon               3.872983    3.872983    4.795832         3.605551   \n",
       "\n",
       "                     Daniel      Nate     Jacob      Anon  \n",
       "Professor Brown    3.162278  2.828427  2.828427  3.464102  \n",
       "Matt Langton       4.898979  4.690416  4.242641  4.898979  \n",
       "Evan               4.358899  3.605551  3.605551  4.123106  \n",
       "Greg Bassett       5.567764  5.567764  5.196152  5.744563  \n",
       "Noah N             5.291503  5.291503  4.690416  5.477226  \n",
       "Tuyetlinh          5.656854  5.477226  5.099020  5.830952  \n",
       "Kenza Bouabdallah  3.316625  3.605551  3.000000  3.872983  \n",
       "Chris Kerfoot      4.795832  4.582576  4.123106  4.795832  \n",
       "Kang Liu           4.123106  3.605551  3.316625  3.872983  \n",
       "Aiden Hill         3.605551  3.605551  3.000000  3.872983  \n",
       "Muhammad S         3.605551  4.582576  3.605551  4.795832  \n",
       "Max Mastrorocco    3.316625  2.645751  3.000000  3.605551  \n",
       "Daniel             0.000000  3.464102  3.464102  4.472136  \n",
       "Nate               3.464102  0.000000  3.464102  4.000000  \n",
       "Jacob              3.464102  3.464102  0.000000  3.741657  \n",
       "Anon               4.472136  4.000000  3.741657  0.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df = pd.DataFrame(data = euclidean_distances(sentence_df),\n",
    "            index= sentence_dict.keys(), columns= sentence_dict.keys())\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd874bd1",
   "metadata": {},
   "source": [
    "Who wrote the most similar question to me?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8ad9de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Max Mastrorocco'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df['Professor Brown'].drop('Professor Brown').idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edca32d",
   "metadata": {},
   "source": [
    "## Questions After Classroom\n",
    "\n",
    "### How can this be used for training a classifier?\n",
    "```{toggle}\n",
    "To train a classifier, we would also need target variables, but the `mat` variable we had above can be used as the `X` for any `sklearn` estimator object.\n",
    "To train more complex tasks you would need appropriate data: for example labeled articles that are real and fake to train a fake news classifier (this is provided for a12).\n",
    "```\n",
    "\n",
    "### How are ram tokens tracked?\n",
    "\n",
    "```{toggle}\n",
    "Ram Tokens are tracked in the [Ram Token Bank: http://drsmb.co/ramtoken](http://drsmb.co/ramtoken) form.  You'll get e-mails when you earn or use them, however no one has submitted for any. You still can though (espeically if you were advised to and forgot).\n",
    "```\n",
    "\n",
    "## More Practice\n",
    "\n",
    "1. Which two people wrote the most similar sentences?\n",
    "1. Do you think this representation captures all cases of similary? Can you generate a case where it doesn't do well?\n",
    "1. Try"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "source_map": [
   12,
   16,
   22,
   48,
   63,
   67,
   81,
   83,
   86,
   88,
   90,
   92,
   104,
   106,
   115,
   120,
   123,
   127,
   129,
   134,
   137,
   140,
   142,
   145,
   150,
   155,
   157,
   161,
   163,
   167,
   169,
   172,
   174,
   176,
   178,
   181,
   183,
   190,
   192,
   195,
   199,
   203,
   205
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}