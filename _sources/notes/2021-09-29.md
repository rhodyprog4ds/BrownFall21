---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# More Reshaping



Continuing from Friday.
```{code-cell} ipython3
import pandas as pd
import seaborn as sns

# make plots look nicer and increase font size
sns.set_theme(font_scale=2)
arabica_data_url = 'https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv'

coffee_df = pd.read_csv(arabica_data_url)

# compute ___ per ___
bag_total_df = coffee_df.groupby('Country.of.Origin')['Number.of.Bags'].sum()

# subset the summary Series for countries with over 15000 total and store as a list
high_prod_countries = list(bag_total_df[bag_total_df>15000].index)

# a lambda function that checks if a string c is one of the
#  countries in high_prod_countries
high_prod = lambda c: c in high_prod_countries

# add a column that indicates that the country is a high producer
coffee_df['high_production'] = coffee_df['Country.of.Origin'].apply(high_prod)



# filter based on production level threshold
high_prod_coffee_df = coffee_df[coffee_df['high_production']]
```

**What happened when we filtered the data?**

```{code-cell} ipython3
coffee_df.shape, high_prod_coffee_df.shape
```
We have many fewer rows.

Now that we've filtered the data. Let's practice reshaping data to by Tidy again.


```{code-cell} ipython3
# replace the FIXMEs
scores_of_interest = ['Balance','Aroma','Body','Aftertaste']
attrs_of_interest = ['Country.of.Origin','Color']
high_prod_coffee_df_melted = high_prod_coffee_df.melt(
    id_vars = attrs_of_interest,
    value_vars = scores_of_interest,
    var_name = 'Score')
```

What happened?
```{code-cell} ipython3
high_prod_coffee_df_melted.shape
```
Now the shape is 4 times as long (because the length of the list we passed to value_vars is 4). And it has 4 columns: the length of the list we passed to `id_vars` + 2 (variable, value)

```{code-cell} ipython3
len(scores_of_interest)
```

```{code-cell} ipython3
len(scores_of_interest)*len(high_prod_coffee_df)
```

We can seee the column names and what they have in them here:

```{code-cell} ipython3
high_prod_coffee_df_melted.head()
```

Note that we passed a value to `var_name` to make that column named "Score".  We could also not pass that

```{code-cell} ipython3
high_prod_coffee_df.melt(
    id_vars = attrs_of_interest,
    value_vars = scores_of_interest)
```
then we have `variable` and `value` as column names.  

```{admonition} Try it yourself
How could you rename the `value` column?
```

The head has only 'Balance' in the 'Score' column, we could use `sample` to pick a random subset of the rows instead to see different values.
```{code-cell} ipython3
high_prod_coffee_df_melted.sample(5)
```


What does this let us do?

One thing is it makes plots easier, because seaborn is organized around tidy data.
```{code-cell} ipython3
sns.displot(data= high_prod_coffee_df_melted,
           x='value',hue='Country.of.Origin',
           col = 'Score', col_wrap=2, kind='kde',aspect =1.5)
```

```{code-cell} ipython3
sns.displot(data= high_prod_coffee_df_melted,
           x='value',hue='Color',
           col = 'Score', col_wrap=2, kind='kde',aspect =1.5)
```


```{code-cell} ipython3
sns.displot(data= high_prod_coffee_df_melted,
           x='value',hue='Country.of.Origin',
           col = 'Score', row='Color', kind='kde',aspect =1.5)
```

(unpacking-jsons)=
## Unpacking Jsons

```{code-cell} ipython3
rhodyprog4ds_gh_events_url = 'https://api.github.com/orgs/rhodyprog4ds/events'
```

```{code-cell} ipython3
course_gh_df = pd.read_json(rhodyprog4ds_gh_events_url)
course_gh_df.head()
```
We want to transform each one of those from a dictionary like thing into a
row in a data frame.

```{code-cell} ipython3
type(course_gh_df['actor'])
```

Recall, that base python types can be used as function, to cast an object from
type to another.
```{code-cell} ipython3
5
```

```{code-cell} ipython3
type(5)
```

```{code-cell} ipython3
str(5)
```

To unpack one column we can cast each element of the column to a series and then stack them back together.

First, let's look at one row of one column
```{code-cell} ipython3
course_gh_df['actor'][0]
```

Now let's cast it to a Series
```{code-cell} ipython3
pd.Series(course_gh_df['actor'][0])
```

What we want is to do this over and over and stack them.

 The `apply` method does this for us, in one compact step.  

```{code-cell} ipython3
course_gh_df['actor'].apply(pd.Series)
```

How can we do this for all of the columns and put them back together after?

First, let's make a list of the columns we need to convert.
```{code-cell} ipython3
js_cols = ['actor','repo','payload','org']
```

When we use `.apply(pd.Series)` we get a a DataFrame.

```{code-cell} ipython3
type(course_gh_df['actor'].apply(pd.Series))
```

`pd.concat` takes a list of DataFrames and puts the together in one DataFrame.

to illustrate, it's nice to make small dataFrames.

```{code-cell} ipython3
df1 = pd.DataFrame([[1,2,3],[3,4,7]], columns = ['A','B','t'])
df2 = pd.DataFrame([[10,20,30],[30,40,70]], columns = ['AA','BB','t'])
df1
```

```{code-cell} ipython3
df2
```

If we use concat with the default settings, it stacks them vertically and aligns any columns that have the same name.  

```{code-cell} ipython3
pd.concat([df1,df2])
```
So, since the original DataFrames were both 2 rows with 3 columns each, with one column name appearing in both, we end up with a new DataFrame with shape (4,5) and it fills with `NaN` in the top right and the bottom left.

```{code-cell} ipython3
pd.concat([df1,df2]).shape
```

We can use the `axis` parameter to tell it how to combine them. The default is `axis=0`, but `axis=1` will combine along rows.
```{code-cell} ipython3
pd.concat([df1,df2], axis =1)
```

So now we get no NaN values, because both DataFrames have the same number of rows and the same index.

```{code-cell} ipython3
df1.index == df2.index
```

and we have a total of 6 columns and 2 rows.

```{code-cell} ipython3
pd.concat([df1,df2], axis =1).shape
```

Back to our gh data, we want to make a list of DataFrames where each DataFrame corresponds to one of the columns in the original DataFrame, but unpacked and then stack them horizontally (`axis=1`) because each DataFrame in the list is based on the same original DataFrame, they again have the same index.
```{code-cell} ipython3
pd.concat([course_gh_df[cur_col].apply(pd.Series) for cur_col in js_cols],
         axis=1)
```

```{admonition} Try it Yourself
examine the list of DataFrames to see what structure they share and do not share
```

In this case we get the same 30 rows, beacuse that's what the API gave us and turned our 4 columns from `js_cols` into 26 columns.
```{code-cell} ipython3
pd.concat([course_gh_df[cur_col].apply(pd.Series) for cur_col in js_cols],
         axis=1).shape
```

If we had used the default, we'd end up with 120 rows (30*4) and we have only 19 columns, because there are subfield names that are shared across the original columns. (eg most have an `id`)
````{margin}
```{admonition} Try it yourself
How could you anticipate how many are shared?
```
````

```{code-cell} ipython3
pd.concat([course_gh_df[cur_col].apply(pd.Series) for cur_col in js_cols],
         axis=0).shape
```

we might want to rename the new columns so that they have the original column
name prepended to the new name. This will help us distinguish between the different `id` columns

pandas has a `rename` method for this.

and this is another job for lambdas.
```{code-cell} ipython3
pd.concat([course_gh_df[cur_col].apply(pd.Series).rename(columns = lambda c: cur_col + '_' +c)
           for cur_col in js_cols],
         axis=1)
```

the `rename` method's `column` parameter can take a lambda defined inline, which is helpful, because we want that function to take one parameter (the current columnt name) and do the same thing to all of the columns within a single DataFrame, but to prepend a different thing for each DataFrame

```{code-cell} ipython3

pd.concat([course_gh_df[cur_col].apply(pd.Series).rename(columns = lambda c: cur_col + '_' +c)
           for cur_col in js_cols],
         axis=1)
```

So now, we have the unpacked columns with good column names, but we lost the columns that were originally good.


How can we append the new columns to the old ones?
First we can make a DataFrame that's just the columns not on the list that we're goign to expand.
```{code-cell} ipython3
course_gf_df_good = course_gh_df[[col for col in
          course_gh_df.columns if not(col in js_cols)]]
course_gf_df_good
```

Then we can prepend that to the list that we pass to `concat`. We have to put it in a list first, then use + to do that.
```{code-cell} ipython3
pd.concat([course_gf_df_good]+[course_gh_df[col].apply(pd.Series,).rename(
  columns= lambda i_col: col + '_' + i_col )
      for col in js_cols],axis=1)
```

To see how the list math works
```{code-cell} ipython3
['a'] + ['b','c','d']
```
results in one list

but without the `[]` we get a type error
```{code-cell} ipython3
'a' + ['b','c','d']
```


List operations return `None` and mutate the list in place so

```{code-cell} ipython3
orig_list = ['a']
new_items = ['b','c','d']
orig_list.extend(new_items)
```
outputs nothing because `None` was returned and it changes the original variable.

```{code-cell} ipython3
orig_list
```


```{code-cell} ipython3
type(orig_list.extend(new_items))
```
is none.


## Questions After Class

All clarifying questions today

### How does Axis work?
```{toggle}
the notes above are expanded a lot, which should help. You can see more examples in the [Tidy Data Explanation](https://rhodyprog4ds.github.io/BrownFall21/notes/2021-09-27.html#tidy-data) and on the [Cheat Sheet](../resources/cheatsheet).  

The `axis` parameter is a parameter in a lot of pandas functions, you can see it used in most of the statistics we used last week as well, because though column operations are the default, we *can* do row-wise as well.

For more on concatenation, see the [Pandas user guide](https://pandas.pydata.org/docs/user_guide/merging.html#concatenating-objects) or [API docs](https://pandas.pydata.org/docs/reference/api/pandas.concat.html#pandas.concat) sections on it.
```

### How does melt work?
```{toggle}
the notes are expanded a lot. Also see [Tidying data](https://rhodyprog4ds.github.io/BrownFall21/notes/2021-09-27.html#tidying-data).

For the concept, you can also see the original [Tidy Data paper](https://www.jstatsoft.org/article/view/v059i10).

For the pandas method, see its [docs](https://pandas.pydata.org/docs/reference/api/pandas.melt.html#pandas.melt).
```

### What about the NaNs that are still left?
````{toggle}
those are Nan in the data because the events are different types and different types of events have different information available about them.

If we `groupby` event type and then look at, for example, the payload columns. We see that the `NaN`s are explained by that. (remember, count tells how many are not `NaN`)
```{code-cell} ipython3
course_gh_df_expanded.groupby('type')[[c
                for c in course_gh_df_expanded.columns if 'payload' in c]].count()
```
````
