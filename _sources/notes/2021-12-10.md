---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  name: python3
---

# Convolutional Neural Netowrks

```{code-cell}

import numpy as np                   # advanced math library
import matplotlib.pyplot as plt     
import random                        # for generating random numbers

from keras.datasets import  cifar10
from keras.models import Sequential  # Model type to be used

from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils                         # NumPy related tools

from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from keras.layers import BatchNormalization
```

```{code-cell}

(c_X_train, c_y_train), (c_X_test, c_y_test) = cifar10.load_data()
```

```{code-cell}


c_X_test.shape
```

```{code-cell}


nb_classes = 10

c_X_train_flat = c_X_train.reshape(50000, 3072) #add an additional dimension to represent the single-channel
c_X_test_flat = c_X_test.reshape(10000, 3072)


c_X_train_flat = c_X_train_flat.astype('float32')         # change integers to 32-bit floating point numbers
c_X_test = c_X_test.astype('float32')

c_X_train_flat /= 255                              # normalize each value for each pixel for the entire vector for each input
c_X_test /= 255

c_Y_train = np_utils.to_categorical(c_y_train, nb_classes)
c_Y_test = np_utils.to_categorical(c_y_test, nb_classes)
```

```{code-cell}


model_dropout = Sequential()

model_dropout.add(Dense(512, input_shape=(3072,)))
model_dropout.add(Activation('sigmoid'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(512))
model_dropout.add(Activation('relu'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(10))
model_dropout.add(Activation('softmax'))
```

```{code-cell}

model_dropout.summary()
```

```{code-cell}

model_dropout.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dropout.fit(c_X_train_flat, c_Y_train,
          batch_size=128, epochs=5,
          verbose=1)

score = model_dropout.evaluate(c_X_test_flat, c_Y_test)
score
```

```{code-cell}

c_X_train = c_X_train.astype('float32')         # change integers to 32-bit floating point numbers
c_X_train = c_X_train.astype('float32')

c_X_train /= 255                              # normalize each value for each pixel for the entire vector for each input
c_X_train /= 255
```

```{code-cell}


model_cnn = Sequential()                                 # Linear stacking of layers

# Convolution Layer 1
model_cnn.add(Conv2D(32, (3, 3), input_shape=(32,32,3))) # 32 different 3x3 kernels -- so 32 feature maps
model_cnn.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer01 = Activation('relu')                     # activation
model_cnn.add(convLayer01)

# Convolution Layer 2
model_cnn.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps
model_cnn.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model_cnn.add(Activation('relu'))                        # activation
convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model_cnn.add(convLayer02)

# Convolution Layer 3
model_cnn.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps
model_cnn.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer03 = Activation('relu')                     # activation
model_cnn.add(convLayer03)

# Convolution Layer 4
model_cnn.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps
model_cnn.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model_cnn.add(Activation('relu'))                        # activation
convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model_cnn.add(convLayer04)
model_cnn.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector

# Fully Connected Layer 5
model_cnn.add(Dense(512))                                # 512 FCN nodes
model_cnn.add(BatchNormalization())                      # normalization
model_cnn.add(Activation('relu'))                        # activation

# Fully Connected Layer 6                       
model_cnn.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes
model_cnn.add(Dense(10))                                 # final 10 FCN nodes
model_cnn.add(Activation('softmax'))                     # softmax activation
```

```{code-cell}

model_cnn.summary()
```

```{code-cell}

model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_cnn.fit(c_X_train, c_Y_train,
          batch_size=128, epochs=5,
          verbose=1)

score = model_cnn.evaluate(c_X_test, c_Y_test)
```

```{code-cell}

score_cnn = model_cnn.evaluate(c_X_test, c_Y_test)
score_cnn
```

```{code-cell}
---
colab:
  background_save: true
id: iEd9uzII6KGs
---

```
