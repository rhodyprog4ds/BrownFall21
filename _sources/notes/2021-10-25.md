---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Linear Regression
```{code-cell} ipython3
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
sns.set_theme(font_scale=2,palette='colorblind')
```

## Setting upa linear regression

```{code-cell} ipython3
tips = sns.load_dataset("tips")
```

```{code-cell} ipython3
tips.head(1)
```

We're going to predict **tip** from **total bill** using 80% of the data for training.
This is a regression problem because the target, *tip* is a continuous value,
the problems we've seen so far were all classification, species of iris and the
character in that corners data were both categorical.  

Using linear regression is also a good choice because it makes sense that the tip
would be approximately linearly related to the total bill, most people pick some
percentage of the total bill.  If we our prior knowledge was that people
typically tipped with some more complicated function, this would not be a good
model.

```{code-cell} ipython3
# sklearn requires 2D object of features even for 1 feature
tips_X = tips['total_bill'].values
tips_X = tips_X[:,np.newaxis] # add an axis
tips_y = tips['tip']

tips_X_train,tips_X_test, tips_y_train, tips_y_test = train_test_split(
                                          tips_X,
                                          tips_y,
                                          train_size=.8,
                                          random_state=0)
```

To see what that new bit of code did, we can examine the shapes:
```{code-cell} ipython3
tips_X.shape
```
what we ended up  is 2 dimensions (there are two numbers) even though the second
one is 1.

```{code-cell} ipython3
tips['total_bill'].values.shape
```
this, without the `newaxis` is one dimension, we can see that because there is
no number after the comma.  

Now that our data is ready, we create the linear regression estimator object

```{code-cell} ipython3
regr = linear_model.LinearRegression()
```

Now we fit the model.
```{code-cell} ipython3
regr.fit(tips_X_train,tips_y_train)
```

We can examine the coefficients and intercept.  
```{code-cell} ipython3
regr.coef_, regr.intercept_
```
These define a line (y = mx+b) coef is the slope.


```{important}
This is what our model *predicts* the tip will be based on the past data.  It is
important to note that this is not what the tip *should* be by any sort of
virtues. For example, a typical normative rule for tipping is to tip 15% or 20%.
the model we learned, from this data, however is ~%10 + $1. (it's actually
9.68% + $1.028)
```

To interpret this, we can apply it for a single value. We trained this to
predict the tip from the total bill.  So, we can put in any value that's a
plausible total bill and get the predicted tip.

```{code-cell} ipython3
my_bill = np.asarray([17.78]).reshape(1,-1)
regr.predict(my_bill)
```


We can also apply the function, as usual.
```{code-cell} ipython3
tips_y_pred = regr.predict(tips_X_test)
```

This gives a vector of values.
```{code-cell} ipython3
tips_y_pred
```

To visualize in more detail, we'll plot the data as black points and the
predictions as blue points.  To highlight that this is a perfectly linear
prediction, we'll also add a line for the prediction.
```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.plot(tips_X_test,tips_y_pred, color='blue')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
```
## Evaluating Regression - Mean Squared Error

From the plot, we can see that there is some error for each point, so accuracy
that we've been using, won't work.  One idea is to look at how much error there
is in each prediction, we can look at that visually first.

```{code-cell} ipython3
plt.scatter(tips_X_test, tips_y_test, color='black')
plt.plot(tips_X_test, tips_y_pred, color='blue', linewidth=3)

# draw vertical lines frome each data point to its predict value
[plt.plot([x,x],[yp,yt], color='red', linewidth=3)
         for x, yp, yt in zip(tips_X_test, tips_y_pred,tips_y_test)];
```
We can use the average length of these red lines to capture the error. To get
the length, we can take the difference between the prediction and the data for
each point. Some would be positive and others negative, so we will square each
one then take the average.  

```{code-cell} ipython3
mean_squared_error(tips_y_test, tips_y_pred)
```

We can get back to the units being dollars, by taking the square root.  
```{code-cell} ipython3
np.sqrt(mean_squared_error(tips_y_test, tips_y_pred))
```

This is equivalent to using absolute value instead
```{code-cell} ipython3
np.mean(np.abs(tips_y_test - tips_y_pred))
```

## Evaluating Regression - R2

We can also use the $R^2$ regression coefficient.

```{code-cell} ipython3
r2_score(tips_y_test,tips_y_pred)
```


This is a bit harder to interpret, but we can use some additional plots to
visualize.
This code simulates data by randomly picking 20 points, spreading them out
and makes the “predicted” y values by picking a slope of 3. Then I simulated various levels of noise, by sampling noise and multiplying the same noise vector by different scales and adding all of those to a data frame with the column name the r score for if that column of target values was the truth.

Then I added some columns of y values that were with different slopes and different functions of x. These all have the small amount of noise.

````{margin}
```{tip}
[Facet Grids](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html) allow more customization than the figure level plotting functions
we have used otherwise, but each of those combines a FacetGrid with a
particular type of plot.
```
````

```{code-cell} ipython3
x = 10*np.random.random(20)
y_pred = 3*x
ex_df = pd.DataFrame(data = x,columns = ['x'])
ex_df['y_pred'] = y_pred
n_levels = range(1,18,2)
# sample 0 mean noise
noise = (np.random.random(20)-.5)*2
# add varying noise levels
for n in n_levels:
    # add noise, scaled
    y_true = y_pred + n* noise
    # compute the r2 in the column name, assign the "true" (data) here
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true

# add functions
f_x_list = [2*x,3.5*x,.5*x**2, .03*x**3, 10*np.sin(x)+x*3,3*np.log(x**2)]
for fx in f_x_list:
    y_true = fx + noise
    # compute the r2 in the column name, assign the "true" (data) here
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true  

# melt the data frame for plotting
xy_df = ex_df.melt(id_vars=['x','y_pred'],var_name='rscore',value_name='y')
# create a FacetGrid so that we can add two types of plots per subplot
g = sns.FacetGrid(data = xy_df,col='rscore',col_wrap=3,aspect=1.5,height=3)
g.map(plt.plot, 'x','y_pred',color='k')
g.map(sns.scatterplot, "x", "y",)
```

## Multivariate Regression

We can also load data from Scikit learn.

This dataset includes 10 features measured on a given date and an measure of
diabetes disease progression measured one year later. The predictor we can train
with this data might be someting a doctor uses to calculate a patient's risk.  
```{code-cell} ipython3
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y = True)
```

```{code-cell} ipython3
diabetes_X.shape
```

```{code-cell} ipython3
diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = train_test_split(
        diabetes_X, diabetes_y)
regr_diabetes = linear_model.LinearRegression()
```

```{code-cell} ipython3
regr_diabetes.fit(diabetes_X_train,diabetes_y_train)
```

## What score does linear regression use?

```{code-cell} ipython3
regr_diabetes.score(diabetes_X_test,diabetes_y_test)
```

```{code-cell} ipython3
diabetes_y_pred = regr_diabetes.predict(diabetes_X_test)
```


```{code-cell} ipython3
r2_score(diabetes_y_test,diabetes_y_pred)
```

```{code-cell} ipython3
mean_squared_error(diabetes_y_test,diabetes_y_pred)
```
It uses the R2 score.  

This model predicts what lab measure a patient will have one year in the future
based on lab measures in a given day.  Since we see that this is not a very high
r2, we can say that this is not a perfect predictor, but a Doctor, who better
understands the score would have to help interpret the core.

## Questions After class

### How I should use these with data most effectively? What is the proper use of these methods?
```{toggle}
To answer continuous prediction tasks, like the ones we saw today. The notes
above include more interpretation than we discussed in class, so read carefully
for that.
```

### Why is that even when random state is set to 0 numbers are still a little different compared to yours and my neighbor even
```{toggle}
[random state](https://scikit-learn.org/stable/glossary.html#term-random_state)
sets the seed that's used internally and should work to
[control the randomness](https://scikit-learn.org/stable/common_pitfalls.html#randomness)
and produce reproducible results.
If your results are just a little different, like that it could be a rounding
error, maybe you somehow set a default for display that's different.

See for example [these options](https://stackoverflow.com/questions/25200609/apply-round-off-setting-to-whole-notebook)
```
