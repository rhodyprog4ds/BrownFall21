---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Predicting with Neural Networks


```{code-cell} ipython3
from scipy.special import expit
from sklearn.datasets import make_classification
from sklearn.neural_network import MLPClassifier

from sklearn import svm
import pandas as pd
import numpy as np
import sklearn

from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn import model_selection
# from skearn.model_selection import train_test_split

import seaborn as sns
sns.set_theme(palette='colorblind')

```



Today, were going to use *very* simple data in order to examin how a neural
network works.  

```{code-cell} ipython3
X, y = make_classification(n_samples=100, random_state=1,n_features=2,n_redundant=0)
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y,
                          random_state=1)
sns.scatterplot(x=X[:,0],y=X[:,1],hue=y)
```

First, we'll train and score a tiny neural net: with 1 hidden layer of 1 neuron.
````{margin}
```{tip}
This is actually eqiuvalent to another classifier, called logistic Regression
```

```{admonition} Correction
I've removed the parameter `learning_rate_init=0.1` and `random_state=1`
 because sklearn actually only
uses the learning rate and randomizaiont for some of the optimization algorithms:
 adam and sgd, which we are not using.

These algorithms are good in high dimensional problems, our problem is simple,
so we don't need the advnaced parameters or algorithms.
```
````

```{code-cell} ipython3
clf = MLPClassifier(
  hidden_layer_sizes=(1), # 1 hidden layer, 1 aritficial neuron
  max_iter=100, # maximum 100 interations in optimization
  alpha=1e-4, # regularization
  solver="lbfgs", #optimization algorithm  
  verbose=10, # how much detail to print
  activation= 'identity' # how to transform the hidden layer beofore passing it to the next layer
)
clf.fit(X_train, y_train)

clf.score(X_test, y_test)
```

Now we can see that it actually has another activation, that we didn't change
the output layer still has a logistic activation layer, which we want.  If we
didn't then the output layer wouldn't be able to be interpretted as a probability,
because probability always needs to be between 0 and 1.
```{code-cell} ipython3
clf.out_activation_
```

The logistic function looks like this:

```{code-cell} ipython3
x_logistic = np.linspace(-10,10,100)
y_logistic = expit(x_logistic)
plt.plot(x_logistic,y_logistic)
```

The fit method learned the following weights:

```{code-cell} ipython3
clf.coefs_
```

and biases
```{code-cell} ipython3
clf.intercepts_
```

These are called coefficients and intercepts because the weights are mutliplied
by the inputs and the biases you can interpret as geometrically as shifting things,
like a line intercept (recall y=mx+b)

## Reconstructing the Predict method

we'll use an acutally new point, we can make one up

```{code-cell} ipython3
type([[-1,2]])
```

we want a numpy array so we will cast it
```{code-cell} ipython3
pt = np.array([[-1,2]])
```

```{code-cell} ipython3
type(pt)
```


numpy's `matmul` does matrix multiplicaion (multiply columns by rows element wise and sum)

$$f(x) = W_2g(W_1^T x +b_1) + b_2 $$

the $g$ is the activation function, which we set to identity $g(x) = x$ so we don't have to do more

```{code-cell} ipython3
np.matmul(pt,clf.coefs_[0]) + clf.intercepts_[0])*clf.coefs_[1] + clf.intercepts_[1])
```

but we're not quite done, the output layer still transforms using the logistic
function, which is also known as [`expit`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html) and we have imported from scipy.

```{code-cell} ipython3
expit((np.matmul(pt,clf.coefs_[0]) + clf.intercepts_[0])*clf.coefs_[1] + clf.intercepts_[1])
```

We can compare this to the classifier's output. It outputs a probability for
each class, we only comptued the probabilyt of the `1` class.

```{code-cell} ipython3
clf.predict_proba(pt)
```

and we can see how it predicts on that point.
```{code-cell} ipython3
clf.predict(pt)
```

A single artificial neuron like the function below. where it has parameters
that have to be determined before we can use it on an input vector.
```{code-cell} ipython3
def aritificial_neuron_template(activation,weights,bias,inputs):
    '''
    simple artificial neuron

    Parameters
    ----------
    activation : function
        activation function of the neuron
    weights : numpy aray
        wights for summing inputs
    bias: numpy array
        bias term added to the weighted sum
    inputs : numpy array
        input to the neuron

    '''
    return activation(np.matmul(inputs,weights) +bias)

# two common activation functions
identity_activation = lambda x: x
logistic_activation = lambda x: expit(x)
```

When we instantiate the multilyer perceptron object, `MLPClassifier`, we pick
the activation function and when we give data to the `fit` method, we get
the weights and biases.

A neural network passes the data to the hidden layer, and the output of the
hidden layer to the output layer.  In our neural network, we have just one neuron
at each layer.

So the `predict_proba` method is the same as the following:

```{code-cell} ipython3
aritificial_neuron_template(logistic_activation,clf.coefs_[1],clf.intercepts_[1],
                 aritificial_neuron_template(identity_activation,clf.coefs_[0],
                   clf.intercepts_[0],pt))

```

To make this easier to read, we can make the intermediate neurons their own
lambda functions.

```{code-cell} ipython3

hidden_neuron = lambda x: aritificial_neuron_template(identity_activation,clf.coefs_[0],clf.intercepts_[0],x)
output_neuron = lambda x: aritificial_neuron_template(expit,clf.coefs_[1],clf.intercepts_[1],x)

output_neuron(hidden_neuron(pt))
```

We can confirm that this works the same as the predict probability method:

```{code-cell} ipython3
clf.predict_proba(pt)
```

## More Features and More Hidden Neurons

First, we'll sample more features and then train a new classifier
```{code-cell} ipython3
X, y = make_classification(n_samples=100, random_state=1,n_features=4,n_redundant=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,
                                                    random_state=5)
pt_4d =np.asarray([[-1,-2,2,-1],[1.5,0,.5,1]])
clf_4d = MLPClassifier(
    hidden_layer_sizes=(1),
    max_iter=5000,
    alpha=1e-4,
    solver="lbfgs",
    verbose=10,
    activation= 'identity'
)

clf_4d.fit(X_train, y_train)


clf_4d.score(X_test, y_test)
```

We can look at this data
```{code-cell} ipython3
df = pd.DataFrame(X,columns=['x0','x1','x2','x3'])
df['y'] = y
sns.pairplot(df,hue='y')
```

and based on this, we'll pick a new pair of points to  test on:
```{code-cell} ipython3
pt_4d =np.asarray([[-2,2,2,-2],[1.5,0,-1,3]])
```

This neural network is just like the one before:
```{code-cell} ipython3
hidden_neuron_4d = lambda x: aritificial_neuron_template(identity_activation,
                                                         clf_4d.coefs_[0],clf_4d.intercepts_[0],x)
output_neuron_4d = lambda x: aritificial_neuron_template(logistic_activation,
                                                         clf_4d.coefs_[1],clf_4d.intercepts_[1],x)


output_neuron_4d(hidden_neuron_4d(pt_4d))
```


```{code-cell} ipython3
clf_4d.predict_proba(pt_4d)
```

However, remember this one was not as accurate:
```{code-cell} ipython3
clf_4d.score(X_test, y_test)
```

To try imporving it, we will add more layers and a different activation function:
```{code-cell} ipython3
clf_4d_4h = MLPClassifier(
    hidden_layer_sizes=(4),
    max_iter=500,
    alpha=1e-4,
    solver="lbfgs",
    verbose=10,
    activation='logistic'
)

clf_4d_4h.fit(X_train, y_train)


clf_4d_4h.score(X_test, y_test)
```
we see some improvment.

This network is more complicated. It has 5 total neurons:

```{code-cell} ipython3
hidden_neuron_4d_h0 = lambda x: aritificial_neuron_template(logistic_activation,
                                                         clf_4d_4h.coefs_[0][:,0],clf_4d_4h.intercepts_[0][0],x)
hidden_neuron_4d_h1 = lambda x: aritificial_neuron_template(logistic_activation,
                                                         clf_4d_4h.coefs_[0][:,1],clf_4d_4h.intercepts_[0][1],x)
hidden_neuron_4d_h2 = lambda x: aritificial_neuron_template(logistic_activation,
                                                         clf_4d_4h.coefs_[0][:,2],clf_4d_4h.intercepts_[0][2],x)
hidden_neuron_4d_h3 = lambda x: aritificial_neuron_template(logistic_activation,
                                                         clf_4d_4h.coefs_[0][:,3],clf_4d_4h.intercepts_[0][3],x)
output_neuron_4d_4h = lambda x: aritificial_neuron_template(logistic_activation,
                                                         clf_4d_4h.coefs_[1],clf_4d_4h.intercepts_[1],x)
```

And we have to take the output of all 4 hidden neurons into the output neuron,
because they are a single layer, not in sequence.

```{code-cell} ipython3

output_neuron_4d_4h(np.asarray([hidden_neuron_4d_h0(pt_4d),
                 hidden_neuron_4d_h1(pt_4d),
                 hidden_neuron_4d_h2(pt_4d),
                 hidden_neuron_4d_h3(pt_4d)]).T)
```

And again, we see this is the probability of predicting 1:
```{code-cell} ipython3
clf_4d_4h.predict_proba(pt_4d)
```





<!-- ```{code-cell} ipython3
clf = MLPClassifier(
  hidden_layer_sizes=(1),
  max_iter=100,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  activation= 'logistic'
)
clf.fit(X_train, y_train)

clf.score(X_test, y_test)
``` -->


## (optional) What is a numerical optimiztion algorithm?

Numerical Optimization algorithms are at the core of many of the fit methods.  

One way we can optimize a function is to take the derivative, set it equal to
zero and sovle for the parameter.  If we know the funciton is convex (like a
  bowl or valley shape) then the place where the derivative (slope) is 0 is
  the bottom or lowest point of the valley.

Numerial optimzaiton is for when we can't analytically solve that problem once
we set it equal to zero. Optimizaiton algorithms are sort of like search algorithms
but can work in high dimensions and use strategy based on calculus.  

The basic idea in many numerical optimization algorithms is to start at a point
(initial setting of the coefficients in this case) and then compute the value
of the function then change the coefficients a little and compute again. We can
use those two point to see if the direction we "moved" or the way we changed the
parameters made it better or worse. If it was better, we change them more in the
same direction, (if we made both smaller then we make them both smaller again)
if it got worse, we change in a different direction.

You can think of this like trying to find the bottom of a valley, without being
able to see, just check your altitude. You take a step left, right, forward or
back and then see if your altitude went up or down.

LBGFS acutally uses the derivative, so it's like you can see the direction of
the hill you're on, but you have to keep taking steps and then if you reacha point
where you can't go down anymore you know you are done.  When the algorithm
finds it can't get better, that's called convergence.

Stochastic gradient descent works in high dimensions where it's too hard to
do the derivative, but you can randomly move in different directions (or take
  the partial derivate in a small numbe rof defintions). Adam is a specical version fo that with better strategy.  


Numerical optimization is a whole research area.  In graduate school, I took a
whole semester long course just learning different algorithms for this.  
