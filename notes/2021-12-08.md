---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  name: python3
---

# Neural Networks with Keras

```{code-cell}

import numpy as np          # advanced math library
import matplotlib.pyplot as plt   
import random            # for generating random numbers

from keras.datasets import mnist  # MNIST dataset is included in Keras  
from keras.models import Sequential # Model type to be used

from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils             # NumPy related tools

from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from keras.layers import BatchNormalization
```

First, we'll use `keras` to build similar networks to the ones we saw with `sklearn`
it will be more complex, and we're using a different version of the digits data
(larger images) but this will still be just learning the predictions, not the
representation for now.  On Friday, we'll learn how to add new layers that
transform the data and learn the representation at the same time.

## Preparing data for deep learning
```{code-cell}

# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

```{code-cell}

plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger

for i in range(9):
  plt.subplot(3,3,i+1)
  num = random.randint(0, len(X_train))
  plt.imshow(X_train[num], cmap='gray', interpolation='none')
  plt.title("Class {}".format(y_train[num]))

plt.tight_layout()
```

```{code-cell}

X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.
X_test = X_test.reshape(10000, 784)  # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.

X_train = X_train.astype('float32')  # change integers to 32-bit floating point numbers
X_test = X_test.astype('float32')

X_train /= 255            # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
```

```{code-cell}

28*28
```

```{code-cell}


nb_classes = 10 # number of unique digits

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
```

```{code-cell}
---

y_train[0]
```

```{code-cell}

Y_train[0]
```

## Building a neural network in Keras

```{code-cell}


model = Sequential()
```

```{code-cell}

model.summary()
```

```{code-cell}


model.add(Dense(512, input_shape=(784,)))
model.add(Activation('relu'))
```

```{code-cell}


model.add(Dense(512))
model.add(Activation('relu'))
```

```{code-cell}


model.add(Dense(10))
model.add(Activation('softmax'))
```

```{code-cell}

model.summary()
```

```{code-cell}
:id: 63_OigzGuoWV

# Let's use the Adam optimizer for learning
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

```{code-cell}

model.fit(X_train, Y_train,
     batch_size=128, epochs=5,
     verbose=1)
```

```{code-cell}

score = model.evaluate(X_test, Y_test)
score
```

## What kind of mistakes did it make?

```{code-cell}

predicted_prob = model.predict(X_test)
predicted_classes = np.argmax(predicted_prob,axis=1)


# Check which items we got right / wrong
correct_indices = np.nonzero(predicted_classes == y_test)[0]

incorrect_indices = np.nonzero(predicted_classes != y_test)[0]


plt.figure()
for i, correct in enumerate(correct_indices[:9]):
  plt.subplot(3,3,i+1)
  plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')
  plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_test[correct]))

plt.tight_layout()

plt.figure()
for i, incorrect in enumerate(incorrect_indices[:9]):
  plt.subplot(3,3,i+1)
  plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
  plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], y_test[incorrect]))

plt.tight_layout()
```


## Dropout for less overfitting

```{code-cell}


model_dropout = Sequential()

model_dropout.add(Dense(512, input_shape=(784,)))
model_dropout.add(Activation('relu'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(512))
model_dropout.add(Activation('relu'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(10))
model_dropout.add(Activation('softmax'))
```

```{code-cell}

model_dropout.summary()
```

```{code-cell}

# Let's use the Adam optimizer for learning
model_dropout.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dropout.fit(X_train, Y_train,
     batch_size=128, epochs=5,
     verbose=1)
```

```{code-cell}

score = model_dropout.evaluate(X_test, Y_test)
score
```

Now we see the gap between the train and test performance is smaller and the
test performance is higher.


## Questions

### ‟What is the difference between the different activation methods?”
